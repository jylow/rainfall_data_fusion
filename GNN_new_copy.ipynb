{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff75aa0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from src.sampling.main import stratified_spatial_sampling_dual, stratified_spatial_kfold_dual\n",
    "from dataset.weather_graph_dataset import WeatherGraphDatasetWithRadar, WeatherGraphDatasetNew\n",
    "\n",
    "import torch\n",
    "from torch_geometric.data import HeteroData\n",
    "from src.raingauge.utils import (\n",
    "    get_station_coordinate_mappings,\n",
    "    load_weather_station_dataset,\n",
    ")\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "import tqdm\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader\n",
    "import time\n",
    "from scipy.stats import pearsonr\n",
    "from src.radar.utils import load_radar_dataset\n",
    "from src.visualization.main import pandas_to_geodataframe, visualise_singapore_outline\n",
    "from src.visualization.radar import (\n",
    "    improved_visualise_radar_grid,\n",
    "    visualize_one_radar_image_with_cropping,\n",
    "    visualize_one_radar_image,\n",
    ")\n",
    "from src.visualization.raingauge import visualise_gauge_grid\n",
    "import cartopy.crs as ccrs\n",
    "import matplotlib as mpl\n",
    "from src.radar.preprocessor import RadarPreprocessor\n",
    "from scipy.spatial import cKDTree\n",
    "from src.miscellaneous import get_straight_distance\n",
    "from models.gnn_radar import HeteroGNN_WithRadar\n",
    "from models.gnn import HeteroGNN\n",
    "import logging\n",
    "from datetime import datetime\n",
    "from src.performance_logger import PerformanceLogger\n",
    "import os\n",
    "from src.utils import add_weather_station_data, add_mask_to_data, generate_edges, add_edge_attributes_to_data, print_data_structure, prepare_dataset_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ae2490d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: Geographic extent of Singapore in longitude and latitude\n",
    "bounds_singapore = {\"left\": 103.6, \"right\": 104.1, \"top\": 1.5, \"bottom\": 1.188}\n",
    "bounds = [0.1, 0.2, 0.5, 1, 2, 4, 7, 10, 20]\n",
    "norm = mpl.colors.BoundaryNorm(boundaries=bounds, ncolors=256, extend=\"both\")\n",
    "\n",
    "experiment_name = f\"{datetime.now().strftime('%Y%m%d_%H%M%S')}_new\"\n",
    "os.makedirs(f\"experiments/{experiment_name}\", exist_ok=True)\n",
    "perf = PerformanceLogger(f\"experiments/{experiment_name}/training_log.jsonl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31bf40e0",
   "metadata": {},
   "source": [
    "# Preprocess Radar Data\n",
    "\n",
    "## Will find common data between radar and weather station"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "962bd24e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"=== Radar Image Preprocessing ===\\n\")\n",
    "\n",
    "# # Load weather station data\n",
    "# # Assuming weather_station_df_pivot is available from your main script\n",
    "# # You'll need to load this from your saved data or regenerate it\n",
    "# try:\n",
    "#     weather_station_data = load_weather_station_dataset(\"weather_station_data.csv\")\n",
    "#     weather_station_locations = get_station_coordinate_mappings()\n",
    "\n",
    "#     cols = list(weather_station_data.columns)\n",
    "#     cols.remove(\"time_sgt\")\n",
    "#     cols.remove(\"gid\")\n",
    "\n",
    "#     weather_station_df_pivot = (\n",
    "#         pd.pivot(data=weather_station_data, index=\"time_sgt\", columns=\"gid\", values=cols)\n",
    "#         .resample(\"15min\")\n",
    "#         .first()\n",
    "#     )\n",
    "\n",
    "#     print(f\"Loaded weather station data: {weather_station_df_pivot.shape[0]} timestamps\")\n",
    "\n",
    "# except Exception as e:\n",
    "#     print(f\"Error loading weather station data: {e}\")\n",
    "\n",
    "# # Initialize preprocessor\n",
    "# preprocessor = RadarPreprocessor(\n",
    "#     radar_base_path=\"database/sg_radar_data\",\n",
    "#     output_path=\"database/sg_radar_data_cropped\",\n",
    "#     weather_station_df=weather_station_df_pivot,\n",
    "# )\n",
    "\n",
    "# # Step 1: Get all radar files\n",
    "# print(\"\\nStep 1: Scanning radar files...\")\n",
    "# radar_files = preprocessor.get_all_radar_files()\n",
    "\n",
    "# if len(radar_files) == 0:\n",
    "#     print(\"No radar files found! Please check the radar_base_path.\")\n",
    "\n",
    "# print(f\"Found {len(radar_files)} radar files\")\n",
    "# print(f\"Date range: {radar_files[0][0]} to {radar_files[-1][0]}\")\n",
    "\n",
    "# # Step 2: Match with weather data\n",
    "# print(\"\\nStep 2: Matching with weather station data...\")\n",
    "# matched_radar_df, matched_weather_df = preprocessor.match_with_weather_data(radar_files)\n",
    "\n",
    "# if len(matched_radar_df) == 0:\n",
    "#     raise ValueError(\"No matching timestamps found!\")\n",
    "\n",
    "# # Step 3: Process and crop all matched files\n",
    "# print(\"\\nStep 3: Cropping radar images to Singapore bounds...\")\n",
    "# results_df = preprocessor.process_all_matched_files(matched_radar_df)\n",
    "\n",
    "# # Step 4: Save metadata\n",
    "# preprocessor.save_metadata(results_df)\n",
    "\n",
    "# # Save matched weather dataframe\n",
    "# matched_weather_path = preprocessor.output_path / \"matched_weather_station_data.csv\"\n",
    "# matched_weather_df.to_csv(matched_weather_path)\n",
    "# print(f\"Matched weather station data saved to: {matched_weather_path}\")\n",
    "\n",
    "# print(\"\\n=== Preprocessing Complete ===\")\n",
    "# print(f\"Cropped radar images saved to: {preprocessor.output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1eb37d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# radar_df = load_radar_dataset(\"sg_radar_data_cropped\", cropped=True)\n",
    "# visualize_one_radar_image(radar_df=radar_df, n=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b5f5279",
   "metadata": {},
   "source": [
    "# Preprocess Radar Station Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4757dbcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = HeteroData()\n",
    "data1 = HeteroData()\n",
    "data2 = HeteroData()\n",
    "data3 = HeteroData()\n",
    "data4 = HeteroData()\n",
    "dtype = torch.float32\n",
    "# radius_km = (\n",
    "#     5  # Depends on the radius we want to connect the radar grid to weather stations\n",
    "# )\n",
    "\n",
    "# # ---- 1) prepare station ID list\n",
    "# weather_station_df_pivot[\"rain_rate\"] *= 12\n",
    "# station_ids = sorted({col[1] for col in weather_station_df_pivot.columns})\n",
    "\n",
    "# # ---- 2) count observations per variable/station\n",
    "# station_counts = weather_station_df_pivot.count().reset_index()\n",
    "# weather_station_info = pd.pivot(station_counts, index=\"gid\", columns=\"level_0\")\n",
    "\n",
    "# # ---- 3) prepare radar features\n",
    "# radar_features, grid_coords, grid_shape = preprocessor.prepare_radar_features_temporal(\n",
    "#     radar_df, weather_station_df_pivot\n",
    "# )\n",
    "\n",
    "# # ---- 4) classify rainfall vs general\n",
    "# rainfall_station = [\n",
    "#     gid for gid, row in weather_station_info.iterrows() if 0 in row.value_counts()\n",
    "# ]\n",
    "\n",
    "# general_station = [\n",
    "#     s for s in weather_station_locations.keys() if s not in rainfall_station\n",
    "# ]\n",
    "\n",
    "# # restrict to stations actually present\n",
    "# rainfall_station = [s for s in rainfall_station if s in weather_station_info.index]\n",
    "# general_station = [s for s in general_station if s in weather_station_info.index]\n",
    "\n",
    "# # ---- 5) build coordinate DF (always consistent)\n",
    "# loc_df = pd.DataFrame.from_dict(\n",
    "#     weather_station_locations, orient=\"index\", columns=[\"latitude\", \"longitude\"]\n",
    "# )\n",
    "\n",
    "# # ---- 6) flatten MultiIndex cols\n",
    "# df_all = weather_station_info.copy()\n",
    "# df_all.columns = [\n",
    "#     \"_\".join([str(c) for c in col])  # tuple -> join\n",
    "#     if isinstance(col, tuple)\n",
    "#     else str(col)  # normal -> stringify\n",
    "#     for col in df_all.columns\n",
    "# ]\n",
    "\n",
    "# # split\n",
    "# df_rain = df_all.loc[rainfall_station].copy()\n",
    "# df_gen = df_all.loc[general_station].copy()\n",
    "\n",
    "# # ---- 7) attach coordinates (index matches)\n",
    "# df_all = df_all.join(loc_df)\n",
    "# df_rain = df_rain.join(loc_df)\n",
    "# df_gen = df_gen.join(loc_df)\n",
    "\n",
    "# # print(f\"DF ALL: \",df_all.head())\n",
    "# # print(f\"DF RAIN: \",df_rain.head())\n",
    "# # print(f\"DF GEN: \",df_gen.head())\n",
    "\n",
    "# # ---- 8) numpy coordinate arrays\n",
    "# rain_coords = df_rain[[\"latitude\", \"longitude\"]].to_numpy()\n",
    "# gen_coords = df_gen[[\"latitude\", \"longitude\"]].to_numpy()\n",
    "# all_coords = df_all[[\"latitude\", \"longitude\"]].to_numpy()\n",
    "\n",
    "# # Convert radius to degrees\n",
    "# # 111.0 km per degree is approximate conversion for latitude\n",
    "# radius_deg = radius_km / 111.0\n",
    "\n",
    "# # ---- 9) Radar to general stations\n",
    "# tree_gen = cKDTree(gen_coords)\n",
    "# radar_to_gen_list = tree_gen.query_ball_point(grid_coords, r=radius_deg)\n",
    "# radar_to_gen_src = []\n",
    "# radar_to_gen_dst = []\n",
    "# radar_to_gen_distances = []\n",
    "\n",
    "# for grid_idx, station_list in enumerate(radar_to_gen_list):\n",
    "#     for station_idx in station_list:\n",
    "#         # Use the same distance function as station-to-station edges\n",
    "#         dist = get_straight_distance(\n",
    "#             grid_coords[grid_idx],  # [lat, lon]\n",
    "#             gen_coords[station_idx],  # [lat, lon]\n",
    "#         )\n",
    "#         if dist <= radius_km:\n",
    "#             radar_to_gen_src.append(grid_idx)\n",
    "#             radar_to_gen_dst.append(station_idx)\n",
    "#             radar_to_gen_distances.append([dist])\n",
    "\n",
    "# print(f\"Radar to general station connections: {len(radar_to_gen_src)}\")\n",
    "\n",
    "# # ---- 10) Radar to rainfall stations\n",
    "# tree_rain = cKDTree(rain_coords)\n",
    "# radar_to_rain_list = tree_rain.query_ball_point(grid_coords, r=radius_deg)\n",
    "# radar_to_rain_src = []\n",
    "# radar_to_rain_dst = []\n",
    "# radar_to_rain_distances = []\n",
    "\n",
    "# for grid_idx, station_list in enumerate(radar_to_rain_list):\n",
    "#     for station_idx in station_list:\n",
    "#         dist = get_straight_distance(\n",
    "#             grid_coords[grid_idx],  # [lat, lon]\n",
    "#             rain_coords[station_idx],  # [lat, lon]\n",
    "#         )\n",
    "#         if dist <= radius_km:\n",
    "#             radar_to_rain_src.append(grid_idx)\n",
    "#             radar_to_rain_dst.append(station_idx)\n",
    "#             radar_to_rain_distances.append([dist])\n",
    "\n",
    "# print(f\"Radar to rainfall station connections: {len(radar_to_rain_src)}\")\n",
    "\n",
    "# # TODO: Since radar grids are not prediction targets, it's latent features / spatial context providers, not necessary to have radar to radar edges\n",
    "# # ---- 11) Radar to radar edges (radius-based, not connectivity-based)\n",
    "# # radar_to_radar_edges = preprocessor.create_grid_edges_radius(grid_coords, radius_km)\n",
    "# # print(f\"Radar to radar edges: {radar_to_radar_edges.shape[1]} edges created\")\n",
    "# # print(f\"First 10 radar-radar edges:\\n{radar_to_radar_edges[:, :10]}\")\n",
    "\n",
    "# # ---- 12) Add radar node features to HeteroData\n",
    "# data[\"radar_grid\"].x = torch.tensor(radar_features, dtype=dtype)\n",
    "# data[\"radar_grid\"].y = torch.tensor(radar_features, dtype=dtype)\n",
    "\n",
    "# # ---- 13) Add edges (with empty array handling)\n",
    "# # Radar to general stations\n",
    "# if len(radar_to_gen_src) > 0:\n",
    "#     data[\"radar_grid\", \"radar_to_gen\", \"general_station\"].edge_index = torch.tensor(\n",
    "#         np.array([radar_to_gen_src, radar_to_gen_dst]), dtype=torch.long\n",
    "#     )\n",
    "#     data[\"general_station\", \"gen_to_radar\", \"radar_grid\"].edge_index = torch.tensor(\n",
    "#         np.array([radar_to_gen_dst, radar_to_gen_src]), dtype=torch.long\n",
    "#     )\n",
    "#     # Add distance attributes\n",
    "#     data[\"radar_grid\", \"radar_to_gen\", \"general_station\"].edge_attr = torch.tensor(\n",
    "#         radar_to_gen_distances, dtype=dtype\n",
    "#     )\n",
    "#     data[\"general_station\", \"gen_to_radar\", \"radar_grid\"].edge_attr = torch.tensor(\n",
    "#         radar_to_gen_distances, dtype=dtype\n",
    "#     )\n",
    "# else:\n",
    "#     print(\"WARNING: No radar to general station edges found within radius\")\n",
    "#     data[\"radar_grid\", \"radar_to_gen\", \"general_station\"].edge_index = torch.empty(\n",
    "#         (2, 0), dtype=torch.long\n",
    "#     )\n",
    "#     data[\"general_station\", \"gen_to_radar\", \"radar_grid\"].edge_index = torch.empty(\n",
    "#         (2, 0), dtype=torch.long\n",
    "#     )\n",
    "#     data[\"radar_grid\", \"radar_to_gen\", \"general_station\"].edge_attr = torch.empty(\n",
    "#         (0, 1), dtype=dtype\n",
    "#     )\n",
    "#     data[\"general_station\", \"gen_to_radar\", \"radar_grid\"].edge_attr = torch.empty(\n",
    "#         (0, 1), dtype=dtype\n",
    "#     )\n",
    "\n",
    "# # Radar to rainfall stations\n",
    "# if len(radar_to_rain_src) > 0:\n",
    "#     data[\"radar_grid\", \"radar_to_rain\", \"rainfall_station\"].edge_index = torch.tensor(\n",
    "#         np.array([radar_to_rain_src, radar_to_rain_dst]), dtype=torch.long\n",
    "#     )\n",
    "#     data[\"rainfall_station\", \"rain_to_radar\", \"radar_grid\"].edge_index = torch.tensor(\n",
    "#         np.array([radar_to_rain_dst, radar_to_rain_src]), dtype=torch.long\n",
    "#     )\n",
    "#     # Add distance attributes\n",
    "#     data[\"radar_grid\", \"radar_to_rain\", \"rainfall_station\"].edge_attr = torch.tensor(\n",
    "#         radar_to_rain_distances, dtype=dtype\n",
    "#     )\n",
    "#     data[\"rainfall_station\", \"rain_to_radar\", \"radar_grid\"].edge_attr = torch.tensor(\n",
    "#         radar_to_rain_distances, dtype=dtype\n",
    "#     )\n",
    "# else:\n",
    "#     print(\"WARNING: No radar to rainfall station edges found within radius\")\n",
    "#     data[\"radar_grid\", \"radar_to_rain\", \"rainfall_station\"].edge_index = torch.empty(\n",
    "#         (2, 0), dtype=torch.long\n",
    "#     )\n",
    "#     data[\"rainfall_station\", \"rain_to_radar\", \"radar_grid\"].edge_index = torch.empty(\n",
    "#         (2, 0), dtype=torch.long\n",
    "#     )\n",
    "#     data[\"radar_grid\", \"radar_to_rain\", \"rainfall_station\"].edge_attr = torch.empty(\n",
    "#         (0, 1), dtype=dtype\n",
    "#     )\n",
    "#     data[\"rainfall_station\", \"rain_to_radar\", \"radar_grid\"].edge_attr = torch.empty(\n",
    "#         (0, 1), dtype=dtype\n",
    "#     )\n",
    "\n",
    "# # ---- 14) Add masks for radar nodes\n",
    "# n_radar_nodes = len(grid_coords)\n",
    "# data[\"radar_grid\"].train_mask = [1 for _ in range(n_radar_nodes)]\n",
    "# data[\"radar_grid\"].val_mask = [1 for _ in range(n_radar_nodes)]\n",
    "# data[\"radar_grid\"].test_mask = [1 for _ in range(n_radar_nodes)]\n",
    "\n",
    "# # ---- 15) Summary\n",
    "# print(\"\\n=== Radar Grid Integration Summary ===\")\n",
    "# print(f\"Radar grid nodes: {n_radar_nodes}\")\n",
    "# print(f\"Grid shape: {grid_shape[0]} x {grid_shape[1]}\")\n",
    "# print(f\"Timesteps: {radar_features.shape[0]}\")\n",
    "# print(f\"Radar to general station edges: {len(radar_to_gen_src)}\")\n",
    "# print(f\"Radar to rainfall station edges: {len(radar_to_rain_src)}\")\n",
    "# print(f\"Station connection radius: {radius_km} km\")\n",
    "# print(f\"Grid connection radius: {radius_km} km\")\n",
    "# print(\"=\" * 40)\n",
    "\n",
    "# print(\"Radar Features: \", radar_features.shape)\n",
    "\n",
    "# perf.log_grid_radius(\n",
    "#     data=data,\n",
    "#     radar_grid_nodes=n_radar_nodes,\n",
    "#     radius_km=radius_km,\n",
    "#     grid_shape=grid_shape,\n",
    "#     radar_to_gen_src=radar_to_gen_src,\n",
    "#     radar_to_rain_src=radar_to_rain_src\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71b30b43",
   "metadata": {},
   "source": [
    "# Preprocess station data.\n",
    "Some stations only contain rainfall information but some stations contain both rainfall and other information.\n",
    "We will split these stations into weather station and general stations \n",
    "\n",
    "Additional info: \n",
    "  Windspeed\n",
    "  Wind Direction\n",
    "  Temperature\n",
    "  Relative Humidity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "173c2616",
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_station_data = load_weather_station_dataset(\"weather_station_data.csv\")\n",
    "weather_station_locations = get_station_coordinate_mappings()\n",
    "print(len(weather_station_locations.keys()))\n",
    "print(len(set(weather_station_data[\"gid\"].values)))\n",
    "cols = list(weather_station_data.columns)\n",
    "cols.remove(\"time_sgt\")\n",
    "cols.remove(\"gid\")\n",
    "\n",
    "weather_station_df_pivot = (\n",
    "    pd.pivot(data=weather_station_data, index=\"time_sgt\", columns=\"gid\", values=cols)\n",
    "    .resample(\"15min\")\n",
    "    .first()\n",
    ")\n",
    "weather_station_df_pivot[\"rain_rate\"] = weather_station_df_pivot[\"rain_rate\"] * 12\n",
    "weather_station_df_counts = weather_station_df_pivot.count().reset_index()\n",
    "\n",
    "weather_station_info = pd.pivot(\n",
    "    data=weather_station_df_counts, index=\"gid\", columns=\"level_0\"\n",
    ")\n",
    "\n",
    "pd.set_option(\"display.max_rows\", None)\n",
    "\n",
    "rainfall_station = [\n",
    "    row[0] for row in weather_station_info.iterrows() if 0 in row[1].value_counts()\n",
    "]\n",
    "general_station = [s for s in weather_station_locations if s not in rainfall_station]\n",
    "\n",
    "print(rainfall_station)\n",
    "print(general_station)\n",
    "count = 0\n",
    "for row in weather_station_df_pivot[\"rain_rate\"].iterrows():\n",
    "    if np.nansum(row[1].to_numpy()) != 0:\n",
    "        count += 1\n",
    "print(f\"Number of timesteps that contain rain: {count}\")\n",
    "print(f\"Total_timesteps = {weather_station_df_pivot.shape[0]}\")\n",
    "\n",
    "# After loading weather_station_df_pivot\n",
    "print(\"--- Station Data Stats ---\")\n",
    "print(weather_station_df_pivot.describe())\n",
    "\n",
    "# # After loading radar data\n",
    "# print(\"\\n--- Radar Data Stats ---\")\n",
    "# print(\n",
    "#     f\"Radar Min: {radar_features.min()}, Radar Max: {radar_features.max()}, Radar Mean: {radar_features.mean()}\"\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1e91f6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "general_station_data = {}\n",
    "rainfall_station_data = {}\n",
    "\n",
    "# TODO: Temporal Data Leakage - Filling missing values in the training set using all data including validation or test set is wrong.\n",
    "# Extract and interpolate station data\n",
    "for station in weather_station_df_pivot.columns.get_level_values(1).unique():\n",
    "    station_cols = (\n",
    "        weather_station_df_pivot.xs(station, level=1, axis=1)\n",
    "        .interpolate(method=\"linear\")\n",
    "        .fillna(method=\"ffill\")\n",
    "        .fillna(method=\"bfill\")\n",
    "    )\n",
    "    if station in general_station:\n",
    "        general_station_data[station] = station_cols.values\n",
    "    else:\n",
    "        rainfall_station_data[station] = station_cols.values[:, 0:1]\n",
    "\n",
    "general_station_temp = [stn for stn in general_station if stn != \"S108\"]\n",
    "general_station = general_station_temp\n",
    "\n",
    "# Prepare features in the correct order\n",
    "general_station_features = []\n",
    "rainfall_station_features = []\n",
    "general_station_ids = []\n",
    "rainfall_station_ids = []\n",
    "\n",
    "for station in general_station:\n",
    "    station_feat = general_station_data[station]\n",
    "    general_station_features.append(station_feat)\n",
    "    general_station_ids.append(station)\n",
    "\n",
    "for station in rainfall_station:\n",
    "    station_feat = rainfall_station_data[station]\n",
    "    rainfall_station_features.append(station_feat)\n",
    "    rainfall_station_ids.append(station)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cdd3e39",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "59c1da65",
   "metadata": {},
   "source": [
    "# Add Station Features to HeteroData Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26124db1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add station features to HeteroData\n",
    "data = add_weather_station_data(data, general_station_features, rainfall_station_features, dtype=dtype)\n",
    "data1 = add_weather_station_data(data1, general_station_features, rainfall_station_features, dtype=dtype)\n",
    "data2 = add_weather_station_data(data2, general_station_features, rainfall_station_features, general_station_ids, rainfall_station_ids, dtype=dtype)\n",
    "data3 = add_weather_station_data(data3, general_station_features, rainfall_station_features, dtype=dtype)\n",
    "data4 = add_weather_station_data(data4, general_station_features, rainfall_station_features, dtype=dtype)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8afab89",
   "metadata": {},
   "source": [
    "# Stratified Spatial Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "853908c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split_info = stratified_spatial_sampling_dual(weather_station_locations, seed=1111)\n",
    "# print(split_info)\n",
    "\n",
    "# data[\"general_station\"].train_mask = [\n",
    "#     1 if station in split_info[\"ml\"][\"train\"] else 0 for station in general_station\n",
    "# ]\n",
    "# data[\"general_station\"].val_mask = [\n",
    "#     1 if station in split_info[\"ml\"][\"validation\"] else 0 for station in general_station\n",
    "# ]\n",
    "# data[\"general_station\"].test_mask = [\n",
    "#     1 if (x == 0 and y == 0) else 0\n",
    "#     for x, y in zip(\n",
    "#         data[\"general_station\"].train_mask, data[\"general_station\"].val_mask\n",
    "#     )\n",
    "# ]\n",
    "\n",
    "# data[\"rainfall_station\"].train_mask = [\n",
    "#     1 if station in split_info[\"ml\"][\"train\"] else 0 for station in rainfall_station\n",
    "# ]\n",
    "# data[\"rainfall_station\"].val_mask = [\n",
    "#     1 if station in split_info[\"ml\"][\"validation\"] else 0\n",
    "#     for station in rainfall_station\n",
    "# ]\n",
    "# data[\"rainfall_station\"].test_mask = [\n",
    "#     1 if (x == 0 and y == 0) else 0\n",
    "#     for x, y in zip(\n",
    "#         data[\"rainfall_station\"].train_mask, data[\"rainfall_station\"].val_mask\n",
    "#     )\n",
    "# ]\n",
    "\n",
    "# print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b62d6977",
   "metadata": {},
   "source": [
    "# Stratified K Fold Spatial Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e7038fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "split_info = stratified_spatial_kfold_dual(weather_station_locations, seed=123, plot=True)\n",
    "print(split_info)\n",
    "\n",
    "data = add_mask_to_data(data, split_info[0], general_station, rainfall_station)\n",
    "data1 = add_mask_to_data(data1, split_info[1], general_station, rainfall_station)\n",
    "data2 = add_mask_to_data(data2, split_info[2], general_station, rainfall_station)\n",
    "data3 = add_mask_to_data(data3, split_info[3], general_station, rainfall_station)\n",
    "data4 = add_mask_to_data(data4, split_info[4], general_station, rainfall_station)\n",
    "\n",
    "print(\"Data: \\n\", data)\n",
    "print(\"Data1: \\n\", data1)\n",
    "print(\"Data2: \\n\", data2)\n",
    "print(\"Data3: \\n\", data3)\n",
    "print(\"Data4: \\n\", data4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3ae33c3",
   "metadata": {},
   "source": [
    "# Edge generation\n",
    "We consider the location of the stations when performing our edge generation. \n",
    "General station locations and rainfall station locations will be considered the same and we will make a connection across the nodes if required. This will ensure that we can connect both the layers together in the graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f5b7b66",
   "metadata": {},
   "outputs": [],
   "source": [
    "K = 4  # Number of neighbors per node\n",
    "edges, edge_attributes = generate_edges(\n",
    "    weather_station_locations,\n",
    "    general_station,\n",
    "    rainfall_station,\n",
    "    K=K,\n",
    ")\n",
    "\n",
    "data = add_edge_attributes_to_data(data, edges, edge_attributes, dtype=dtype)\n",
    "data1 = add_edge_attributes_to_data(data1, edges, edge_attributes, dtype=dtype)\n",
    "data2 = add_edge_attributes_to_data(data2, edges, edge_attributes, dtype=dtype)\n",
    "data3 = add_edge_attributes_to_data(data3, edges, edge_attributes, dtype=dtype)\n",
    "data4 = add_edge_attributes_to_data(data4, edges, edge_attributes, dtype=dtype)\n",
    "\n",
    "print(data)\n",
    "print(data1)\n",
    "print(data2)\n",
    "print(data3)\n",
    "print(data4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14b1466f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# FINAL SUMMARY\n",
    "# ============================================================================\n",
    "print_data_structure(data)\n",
    "print_data_structure(data1)\n",
    "print_data_structure(data2)\n",
    "print_data_structure(data3)\n",
    "print_data_structure(data4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf3e9cf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(len(edge_attributes[\"rainfall_to_rainfall\"]))\n",
    "\n",
    "# # Process edge indices\n",
    "# print(data)\n",
    "# print(data.edge_types)\n",
    "\n",
    "# print(data[\"general_station\", \"gen_to_rain\", \"rainfall_station\"].edge_attr)\n",
    "# print(data[\"rainfall_station\", \"rain_to_gen\", \"general_station\"].edge_index)\n",
    "# print(data[\"general_station\", \"gen_to_gen\", \"general_station\"].edge_index)\n",
    "# print(\n",
    "#     len(\n",
    "#         set(\n",
    "#             data[\"rainfall_station\", \"rain_to_rain\", \"rainfall_station\"]\n",
    "#             .edge_index.detach()\n",
    "#             .numpy()[0]\n",
    "#         )\n",
    "#     )\n",
    "# )\n",
    "\n",
    "# print(data.has_isolated_nodes())\n",
    "# print(data.has_self_loops())\n",
    "# print(data.is_undirected())\n",
    "\n",
    "# print(data[\"general_station\", \"gen_to_rain\", \"rainfall_station\"][\"edge_index\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f959a81",
   "metadata": {},
   "source": [
    "# Creating the GNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "131359f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = HeteroGNN(hidden_channels=8, out_channels=1, num_layers=5)\n",
    "model1 = HeteroGNN(hidden_channels=8, out_channels=1, num_layers=5)\n",
    "model2 = HeteroGNN(hidden_channels=8, out_channels=1, num_layers=5)\n",
    "model3 = HeteroGNN(hidden_channels=8, out_channels=1, num_layers=5)\n",
    "model4 = HeteroGNN(hidden_channels=8, out_channels=1, num_layers=5)\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model.to(device=device)\n",
    "model1.to(device=device)\n",
    "model2.to(device=device)\n",
    "model3.to(device=device)\n",
    "model4.to(device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c5f28c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, data, dataloader, optimizer, device, verbose=False, log_file=\"training_gnn_new_debug.log\"):\n",
    "    \"\"\"Training loop with radar data.\"\"\"\n",
    "    model.train()\n",
    "    epoch_losses = []\n",
    "\n",
    "    charge_bar = tqdm.tqdm(dataloader, desc='training')\n",
    "\n",
    "    # Setup logging if verbose\n",
    "    if verbose:\n",
    "        logger = logging.getLogger(\"train_debug\")\n",
    "        logger.setLevel(logging.INFO)\n",
    "        if not logger.handlers:\n",
    "            fh = logging.FileHandler(log_file, mode=\"a\")\n",
    "            fh.setFormatter(logging.Formatter(\"%(asctime)s - %(message)s\"))\n",
    "            logger.addHandler(fh)\n",
    "        logger.info(f\"=== Training Epoch Debug Log Started at {datetime.now()} ===\")\n",
    "\n",
    "    for batch in charge_bar:\n",
    "        # Reset gradients ONCE per batch\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        edge_index_dict = batch[\"edge_index_dict\"]\n",
    "        edge_attribute_dict = batch[\"edge_attr_dict\"]\n",
    "\n",
    "        train_metastation_mask = torch.tensor(batch['metastation_mask'], dtype=torch.bool).to(device)\n",
    "        train_rainfallstation_mask = torch.tensor(batch['rainfallstation_mask'], dtype=torch.bool).to(device)\n",
    "\n",
    "        # Array containing MSE array\n",
    "        # Should be list of tensor[stations, 1] of size batch which will be used to calculate average MSE for each timestamp\n",
    "        batch_MSE_arr = [] \n",
    "\n",
    "\n",
    "        training_metastation_indices = train_metastation_mask.nonzero(as_tuple=False)\n",
    "        training_rainfallstation_indices = train_rainfallstation_mask.nonzero(as_tuple=False)\n",
    "            \n",
    "        gen_x = batch['gen_x']\n",
    "        rain_x = batch['rain_x']\n",
    "        gen_y = batch['gen_y']\n",
    "        rain_y = batch['rain_y']\n",
    "\n",
    "        # Perform mask one\n",
    "        for idx in training_metastation_indices:\n",
    "            gen_x_masked = gen_x.clone()\n",
    "            rain_x_masked = rain_x.clone()\n",
    "        \n",
    "            gen_x_masked[:, ~train_metastation_mask.bool()] = 0\n",
    "            rain_x_masked[:, ~train_rainfallstation_mask.bool()] = 0\n",
    "            gen_x_masked[:, idx, 0] = 0\n",
    "\n",
    "            x_dict = {\n",
    "                \"general_station\": gen_x_masked,\n",
    "                \"rainfall_station\": rain_x_masked,\n",
    "            }\n",
    "                \n",
    "                # NO optimizer.zero_grad() here!\n",
    "            out = model(x_dict, edge_index_dict, edge_attribute_dict)\n",
    "\n",
    "            gen_predictions = out['general_station'][:, idx]\n",
    "            gen_actual = gen_y[:, idx]\n",
    "\n",
    "            MSE_arr = (gen_predictions.squeeze() - gen_actual.squeeze()) ** 2\n",
    "            batch_MSE_arr.append(MSE_arr)\n",
    "                \n",
    "            # if verbose:\n",
    "            #     log_msg = f\"\"\"\n",
    "            #         --- DEBUG STATS (General Station) ---\n",
    "            #         Loss for this sample: {training_loss.item():.2f}\n",
    "            #         \"\"\"\n",
    "            #     logger.info(log_msg)\n",
    "                \n",
    "\n",
    "            # Process rainfall stations\n",
    "        for idx in training_rainfallstation_indices:\n",
    "            gen_x_masked = gen_x.clone()\n",
    "            rain_x_masked = rain_x.clone()\n",
    "    \n",
    "            gen_x_masked[:, ~train_metastation_mask.bool()] = 0\n",
    "            rain_x_masked[:, ~train_rainfallstation_mask.bool()] = 0\n",
    "            rain_x_masked[:, idx, 0] = 0\n",
    "\n",
    "            x_dict = {\n",
    "                'general_station': gen_x_masked,\n",
    "                'rainfall_station': rain_x_masked,\n",
    "            }\n",
    "\n",
    "            # NO optimizer.zero_grad() here!\n",
    "            out = model(x_dict, edge_index_dict, edge_attribute_dict)\n",
    "\n",
    "            rain_predictions = out['rainfall_station'][:, idx]\n",
    "            rainfall_actual = rain_y[:, idx]\n",
    "\n",
    "            MSE_arr = (rain_predictions.squeeze() - rainfall_actual.squeeze()) ** 2\n",
    "            \n",
    "            # if verbose:\n",
    "            #     log_msg = f\"\"\"\n",
    "            #         --- DEBUG STATS (Rainfall Station) ---\n",
    "            #         Loss for this sample: {training_loss.item():.2f}\n",
    "            #         \"\"\"\n",
    "            #     logger.info(log_msg)\n",
    "            \n",
    "            batch_MSE_arr.append(MSE_arr)\n",
    "        \n",
    "        # Average loss across all masked stations in batch\n",
    "        losses = torch.stack(batch_MSE_arr).mean()\n",
    "        loss = torch.mean(torch.mean(losses, dim=0))\n",
    "        epoch_losses.append(loss)\n",
    "\n",
    "        # Backpropagate ONCE per batch\n",
    "        loss.backward()\n",
    "\n",
    "        # Update weights ONCE per batch\n",
    "        optimizer.step()\n",
    "    epoch_loss = torch.stack(epoch_losses).mean().item()\n",
    "\n",
    "    return epoch_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c26809b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(model, data, dataloader, device):\n",
    "    \"\"\"Validation loop with radar data.\"\"\"\n",
    "    model.eval()  # Set to eval mode\n",
    "    validation_loss_arr = []\n",
    "\n",
    "    # Prepare masks\n",
    "    val_gen_mask = torch.tensor(\n",
    "        data[\"general_station\"].val_mask, \n",
    "        dtype=torch.bool\n",
    "    ).to(device)\n",
    "    val_rain_mask = torch.tensor(\n",
    "        data[\"rainfall_station\"].val_mask, \n",
    "        dtype=torch.bool\n",
    "    ).to(device)\n",
    "\n",
    "\n",
    "    # Move edge data to device\n",
    "    edge_index_dict = {key: val.to(device) for key, val in data.edge_index_dict.items()}\n",
    "    edge_attr_dict = {key: val.to(device) for key, val in data.edge_attr_dict.items()}\n",
    "\n",
    "    with torch.no_grad():  # No gradients during validation\n",
    "        for batch in tqdm.tqdm(dataloader, desc=\"Validation\"):\n",
    "            gen_x = batch[\"gen_x\"].to(device)\n",
    "            rain_x = batch[\"rain_x\"].to(device)\n",
    "            gen_y = batch[\"gen_y\"].to(device)\n",
    "            rain_y = batch[\"rain_y\"].to(device)\n",
    "\n",
    "            batch_loss = 0\n",
    "\n",
    "            # Mask train and test stations for validation\n",
    "            gen_x_masked = gen_x.clone()\n",
    "            rain_x_masked = rain_x.clone()\n",
    "\n",
    "            gen_x_masked[:, ~val_gen_mask] = 0\n",
    "            rain_x_masked[:, ~val_rain_mask] = 0\n",
    "\n",
    "            # Create input dictionary with radar data\n",
    "            x_dict = {\n",
    "                \"general_station\": gen_x_masked,\n",
    "                \"rainfall_station\": rain_x_masked,\n",
    "            }\n",
    "\n",
    "            out = model(x_dict, edge_index_dict, edge_attr_dict)\n",
    "\n",
    "                # Calculate loss only on validation stations\n",
    "            gen_predictions = out[\"general_station\"][:, val_gen_mask]\n",
    "            rain_predictions = out[\"rainfall_station\"][:, val_rain_mask]\n",
    "\n",
    "            gen_actual = gen_y[:, val_gen_mask]\n",
    "            rain_actual = rain_y[:, val_rain_mask]\n",
    "\n",
    "            preds = torch.cat([gen_predictions, rain_predictions], dim=1)\n",
    "            actuals = torch.cat([gen_actual, rain_actual], dim=1)\n",
    "\n",
    "            SE = (preds - actuals) ** 2\n",
    "            batch_loss = torch.mean(torch.mean(SE))\n",
    "                              \n",
    "            validation_loss_arr.append(batch_loss)\n",
    "\n",
    "    validation_loss = torch.mean(torch.stack(validation_loss_arr)).item()\n",
    "    print(validation_loss)\n",
    "\n",
    "    return validation_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c428c47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set seeds\n",
    "seed = 123\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "perf.log_model_config(model.config)\n",
    "\n",
    "batch_size = 16\n",
    "train_loader, val_loader = prepare_dataset_new(data, batch_size=batch_size)\n",
    "train_loader1, val_loader1 = prepare_dataset_new(data1, batch_size=batch_size)\n",
    "train_loader2, val_loader2 = prepare_dataset_new(data2, batch_size=batch_size)\n",
    "train_loader3, val_loader3 = prepare_dataset_new(data3, batch_size=batch_size)\n",
    "train_loader4, val_loader4 = prepare_dataset_new(data4, batch_size=batch_size)\n",
    "\n",
    "def train(model, data, train_loader, val_loader, fold, device=\"cpu\"):\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "    training_loss_arr = []\n",
    "    validation_loss_arr = []\n",
    "    early = 0\n",
    "    mini = 1000\n",
    "    stopping_condition = 5\n",
    "    epochs = 0\n",
    "    total_epochs = 10\n",
    "    print(f\"-----FOLD: {fold}-----\")\n",
    "    training_start = time.time()\n",
    "    for i in range(total_epochs):\n",
    "        print(f\"-----EPOCH: {i + 1}-----\")\n",
    "        train_loss = train_epoch(\n",
    "            model, data, train_loader, optimizer, device, verbose=False\n",
    "        )\n",
    "        validation_loss = validate(model, data, val_loader, device)\n",
    "        training_loss_arr.append(train_loss)\n",
    "        validation_loss_arr.append(validation_loss)\n",
    "        perf.log_epoch(i, train_loss, validation_loss)\n",
    "        if mini >= validation_loss:\n",
    "            mini = validation_loss\n",
    "            early = 0\n",
    "        else:\n",
    "            early += 1\n",
    "        epochs += 1\n",
    "        if early >= stopping_condition:\n",
    "            print(\"Early stop loss\")\n",
    "            break\n",
    "\n",
    "        print(f\"Train Loss: {train_loss:.4f}\")\n",
    "        print(f\"Validation Loss: {validation_loss:.4f}\")\n",
    "\n",
    "    training_end = time.time()\n",
    "    total_time = training_end - training_start\n",
    "    perf.finalise(total_time)\n",
    "\n",
    "    print(f\"Training took {total_time} seconds over {epochs} epochs\")\n",
    "    plt.plot(training_loss_arr, label=\"training_loss\", color=\"blue\")\n",
    "    plt.plot(validation_loss_arr, label=\"validation_loss\", color=\"red\")\n",
    "    plt.legend()\n",
    "    plt.savefig(f\"experiments/{experiment_name}/train_loss_plot_{fold}.png\", dpi=300)\n",
    "    plt.close()\n",
    "\n",
    "    torch.save(model.state_dict(), f\"experiments/{experiment_name}/weather_gnn_best_{fold}.pth\")\n",
    "    print(\"âœ… model weights saved to weather_gnn_best.pth\")\n",
    "\n",
    "    perf.log_model_parameters(model)\n",
    "    return model\n",
    "\n",
    "model = train(model, data, train_loader, val_loader, fold=0, device=device)\n",
    "model1 = train(model1, data1, train_loader1, val_loader1, fold=1, device=device)\n",
    "model2 = train(model2, data2, train_loader2, val_loader2, fold=2, device=device)\n",
    "model3 = train(model3, data3, train_loader3, val_loader3, fold=3, device=device)\n",
    "model4 = train(model4, data4, train_loader4, val_loader4, fold=4, device=device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c18fdc00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(next(iter(val_loader))[\"gen_x\"].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "623fbdbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# total_params = sum(param.numel() for param in model.parameters())\n",
    "# print(total_params)\n",
    "# print(list(param for param in model.parameters()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35259636",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(model, data, device, collate_fn, fold=0):\n",
    "    model.eval()\n",
    "    total_rmse = 0\n",
    "\n",
    "    # plot_preds = np.array([])\n",
    "    # plot_actual = np.array([])\n",
    "    plot_preds = []\n",
    "    plot_actual = []\n",
    "    gen_predictions_all = []\n",
    "    rain_predictions_all = []\n",
    "    gen_truth_all = []\n",
    "    rain_truth_all = []\n",
    "\n",
    "    station_ids_list = []\n",
    "    timestep_list = []\n",
    "\n",
    "    test_dataset = WeatherGraphDatasetNew(data, mode=\"test\")\n",
    "\n",
    "    test_gen_mask = torch.tensor(\n",
    "        data[\"general_station\"].test_mask, dtype=torch.bool\n",
    "    ).to(device)\n",
    "    test_rain_mask = torch.tensor(\n",
    "        data[\"rainfall_station\"].test_mask, dtype=torch.bool\n",
    "    ).to(device)\n",
    "\n",
    "    # NEW: Get actual station IDs for test nodes\n",
    "    gen_test_station_ids = np.array(data[\"general_station\"].station_ids)[test_gen_mask.cpu().numpy()]\n",
    "    rain_test_station_ids = np.array(data[\"rainfall_station\"].station_ids)[test_rain_mask.cpu().numpy()]\n",
    "    station_ids_list = np.concat([gen_test_station_ids, rain_test_station_ids])\n",
    "\n",
    "    edge_index_dict = {key: val.to(device) for key, val in data.edge_index_dict.items()}\n",
    "    edge_attr_dict = {key: val.to(device) for key, val in data.edge_attr_dict.items()}\n",
    "\n",
    "    test_dataloader = DataLoader(\n",
    "        test_dataset,\n",
    "        batch_size=16,\n",
    "        shuffle=False,\n",
    "        collate_fn=collate_fn,  # Use the passed-in collate_fn\n",
    "    )\n",
    "\n",
    "    count = 0\n",
    "    timestep = 0\n",
    "    printed_stats = False  # Flag to print stats only once\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm.tqdm(test_dataloader, desc=\"Testing\"):\n",
    "            gen_x = batch[\"gen_x\"].to(device)\n",
    "            rain_x = batch[\"rain_x\"].to(device)\n",
    "            gen_y = batch[\"gen_y\"].to(device)\n",
    "            rain_y = batch[\"rain_y\"].to(device)\n",
    "\n",
    "            batch_rmse = 0\n",
    "\n",
    "            gen_x_masked = gen_x.clone()\n",
    "            rain_x_masked = rain_x.clone()\n",
    "\n",
    "            # Mask out all non-training nodes, as done in training\n",
    "            gen_x_masked[:, ~test_gen_mask] = 0\n",
    "            rain_x_masked[:, ~test_rain_mask] = 0\n",
    "\n",
    "            x_dict = {\n",
    "                \"general_station\": gen_x_masked,\n",
    "                \"rainfall_station\": rain_x_masked,\n",
    "            }\n",
    "\n",
    "            out = model(x_dict, edge_index_dict, edge_attr_dict)\n",
    "\n",
    "            # 3. Calculate predictions and targets on TEST nodes\n",
    "            gen_predictions = out[\"general_station\"][:, test_gen_mask]\n",
    "            rain_predictions = out[\"rainfall_station\"][:, test_rain_mask]\n",
    "\n",
    "            gen_targets = gen_y[:, test_gen_mask]\n",
    "            rain_targets = rain_y[:, test_rain_mask]\n",
    "\n",
    "            # ==== DEBUGGING PRINT BLOCK ====\n",
    "            if not printed_stats :\n",
    "                print(\"\\n--- DEBUG STATS (First Sample of Test Set) ---\")\n",
    "\n",
    "                print(\"\\n--- INPUT FEATURES (Masked) ---\")\n",
    "                print(\n",
    "                    f\"General station features:  Min={x_dict['general_station'].min():.2f}, Max={x_dict['general_station'].max():.2f}, Mean={x_dict['general_station'].mean():.2f}\"\n",
    "                )\n",
    "\n",
    "                print(\"\\n--- MODEL PREDICTIONS (for TEST nodes) ---\")\n",
    "                if gen_predictions.numel() > 0:\n",
    "                    print(\n",
    "                        f\"Pred (gen) tensor:    Min={gen_predictions.min():.2f}, Max={gen_predictions.max():.2f}, Mean={gen_predictions.mean():.2f}\"\n",
    "                    )\n",
    "                else:\n",
    "                    print(\"Pred (gen) tensor is empty for this batch.\")\n",
    "\n",
    "                if rain_predictions.numel() > 0:\n",
    "                    print(\n",
    "                        f\"Pred (rain) tensor:   Min={rain_predictions.min():.2f}, Max={rain_predictions.max():.2f}, Mean={rain_predictions.mean():.2f}\"\n",
    "                    )\n",
    "                else:\n",
    "                    print(\"Pred (rain) tensor is empty for this batch.\")\n",
    "\n",
    "                print(\"\\n--- GROUND TRUTH (for TEST nodes) ---\")\n",
    "                if gen_targets.numel() > 0:\n",
    "                    print(\n",
    "                        f\"Truth (gen) tensor:    Min={gen_targets.min():.2f}, Max={gen_targets.max():.2f}, Mean={gen_targets.mean():.2f}\"\n",
    "                    )\n",
    "                else:\n",
    "                    print(\"Truth (gen) tensor is empty for this batch.\")\n",
    "\n",
    "                if rain_targets.numel() > 0:\n",
    "                    print(\n",
    "                        f\"Truth (rain) tensor:   Min={rain_targets.min():.2f}, Max={rain_targets.max():.2f}, Mean={rain_targets.mean():.2f}\"\n",
    "                    )\n",
    "                else:\n",
    "                    print(\"Truth (rain) tensor is empty for this batch.\")\n",
    "\n",
    "                printed_stats = True\n",
    "            # ===============================\n",
    "            gen_predictions_all.append(gen_predictions)\n",
    "            rain_predictions_all.append(rain_predictions)\n",
    "            gen_truth_all.append(gen_targets)\n",
    "            rain_truth_all.append(rain_targets)\n",
    "            \n",
    "    print(\"DEBUG: PRINTS\")\n",
    "    gen_predictions_all = torch.stack(gen_predictions_all)\n",
    "    rain_predictions_all = torch.stack(rain_predictions_all)\n",
    "    gen_truth_all = torch.stack(gen_truth_all)\n",
    "    rain_truth_all = torch.stack(rain_truth_all)\n",
    "\n",
    "    #Contains tensor of all the values for the prediction and truth. \n",
    "    #The tensor was flattened from [number of batches, batch_size, stations, 1] to [timestamps, stations, 1] to make it easier to store values\n",
    "    predictions_all = torch.cat([gen_predictions_all, rain_predictions_all], dim = 2).flatten(0,1)\n",
    "    truth_all = torch.cat([gen_truth_all, rain_truth_all], dim = 2).flatten(0,1)\n",
    "\n",
    "    print(predictions_all.shape)\n",
    "    print(truth_all.shape)\n",
    "\n",
    "    plot_preds = predictions_all.flatten().numpy()\n",
    "    plot_actual = truth_all.flatten().numpy()\n",
    "\n",
    "    mask = ~np.isnan(plot_actual) & ~np.isnan(\n",
    "        plot_preds\n",
    "    )  # Also check for NaNs in preds\n",
    "    pearson_r_global, pearson_p_global = pearsonr(plot_actual[mask], plot_preds[mask])\n",
    "\n",
    "    print(f\"Pearson correlation (Test Stations): {pearson_r_global}\")\n",
    "\n",
    "\n",
    "    #Final RMSE is the average RMSE across each of the timestamps\n",
    "    mse = (predictions_all - truth_all) ** 2\n",
    "    rmse_arr = torch.sqrt(torch.mean(mse, dim = 1))\n",
    "    print(rmse_arr.shape)\n",
    "    final_rmse = torch.mean(rmse_arr)\n",
    "    print(f\"Final Test RMSE (Test Stations): {final_rmse}\")\n",
    "\n",
    "    perf.log_test_metrics(final_rmse, pearson_r_global)\n",
    "\n",
    "    # Convert lists of arrays into one big array\n",
    "    gen_truth_all_np = gen_truth_all.flatten().numpy()\n",
    "    rain_truth_all_np = rain_truth_all.flatten().numpy()\n",
    "    gen_predictions_all_np = gen_predictions_all.flatten().numpy()\n",
    "    rain_predictions_all_np = rain_predictions_all.flatten().numpy()\n",
    "\n",
    "    print(gen_truth_all.shape)\n",
    "\n",
    "    # Now they are NumPy arrays, so flatten() works fine\n",
    "    results_df = pd.DataFrame({\n",
    "        \"fold\": fold,\n",
    "        \"station_type\": ([\"general\"] * len(gen_predictions_all_np.flatten())) \n",
    "                        + ([\"rainfall\"] * len(rain_predictions_all_np.flatten())),\n",
    "        \"true_value\": np.concatenate([\n",
    "            gen_truth_all.flatten(),\n",
    "            rain_truth_all.flatten()\n",
    "        ], axis=0),\n",
    "        \"pred_value\": np.concatenate([\n",
    "            gen_predictions_all_np.flatten(),\n",
    "            rain_predictions_all_np.flatten()\n",
    "        ], axis=0),\n",
    "    })\n",
    "\n",
    "    # Compute absolute error\n",
    "    results_df[\"abs_error\"] = np.abs(results_df[\"pred_value\"] - results_df[\"true_value\"])\n",
    "\n",
    "    # --- Mark cases where actual > 0 but predicted near zero ---\n",
    "    results_df[\"is_false_zero\"] = (\n",
    "        (results_df[\"true_value\"] > 0) & (results_df[\"pred_value\"].between(0, 2))\n",
    "    )\n",
    "\n",
    "    # --- Save all results ---\n",
    "    os.makedirs(f\"experiments/{experiment_name}/predictions\", exist_ok=True)\n",
    "    results_df.to_csv(\n",
    "        f\"experiments/{experiment_name}/predictions/all_predictions_fold{fold}.csv\", \n",
    "        index=False\n",
    "    )\n",
    "\n",
    "    # --- Save subset of \"false zero\" predictions ---\n",
    "    false_zero_df = results_df[results_df[\"is_false_zero\"]]\n",
    "    false_zero_df.to_csv(\n",
    "        f\"experiments/{experiment_name}/predictions/false_zeros_fold{fold}.csv\", \n",
    "        index=False\n",
    "    )\n",
    "\n",
    "    # print(fold)\n",
    "    # print(station_ids_list)\n",
    "    # print(timestep_list)\n",
    "    # print(plot_actual)\n",
    "    # print(plot_preds)\n",
    "    # # --- Save station results ---\n",
    "    # station_results_df = pd.DataFrame({\n",
    "    #     \"fold\": fold,\n",
    "    #     \"station_id\": station_ids_list,\n",
    "    #     \"timestep\": timestep_list,\n",
    "    #     \"true_value\": plot_actual,\n",
    "    #     \"pred_value\": plot_preds\n",
    "    # })\n",
    "    # # Compute absolute error\n",
    "    # station_results_df[\"abs_error\"] = np.abs(station_results_df[\"pred_value\"] - station_results_df[\"true_value\"])\n",
    "\n",
    "    # # Mark cases where actual > 0 but predicted near zero\n",
    "    # station_results_df[\"is_false_zero\"] = (\n",
    "    #     (station_results_df[\"true_value\"] > 0) & (station_results_df[\"pred_value\"].between(0, 2))\n",
    "    # )\n",
    "    # station_results_df.to_csv(\n",
    "    #     f\"experiments/{experiment_name}/predictions/station_predictions_fold{fold}.csv\", \n",
    "    #     index=False\n",
    "    # )\n",
    "\n",
    "    # # --- Save station errors ---\n",
    "    # station_errors = station_results_df.groupby(\"station_id\").agg({\n",
    "    #     \"abs_error\": [\"mean\", \"median\", \"max\", \"std\"],\n",
    "    #     \"true_value\": \"mean\",\n",
    "    #     \"pred_value\": \"mean\"\n",
    "    # }).reset_index()\n",
    "    \n",
    "    # station_errors.columns = [\"station_id\", \"mean_abs_error\", \"median_abs_error\", \n",
    "    #                            \"max_abs_error\", \"std_abs_error\", \"mean_true\", \"mean_pred\"]\n",
    "    # station_errors[\"bias\"] = station_errors[\"mean_pred\"] - station_errors[\"mean_true\"]\n",
    "    # station_errors[\"fold\"] = fold\n",
    "    \n",
    "    # station_errors.to_csv(\n",
    "    #     f\"experiments/{experiment_name}/predictions/station_errors_fold{fold}.csv\",\n",
    "    #     index=False\n",
    "    # )\n",
    "\n",
    "    # print(f\"\\nSaved prediction results for fold {fold}:\")\n",
    "    # print(f\"  â†’ all_predictions_fold{fold}.csv  ({len(results_df)} samples)\")\n",
    "    # print(f\"  â†’ station_errors_fold{fold}.csv  ({len(station_errors)} stations)\")\n",
    "    # print(f\"  â†’ false_zeros_fold{fold}.csv  ({len(false_zero_df)} false zeros)\")\n",
    "    \n",
    "    # print(\"\\nTop 5 worst stations by mean absolute error:\")\n",
    "    # print(station_errors.nlargest(5, \"mean_abs_error\")[[\"station_id\", \"mean_abs_error\", \"bias\"]])\n",
    "\n",
    "    # --- Plotting and Final Metrics ---\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    plt.scatter(x=plot_actual, y=plot_preds, alpha=0.5)\n",
    "\n",
    "    plot_bound = max(\n",
    "        np.nanmax(plot_actual).astype(int), np.nanmax(plot_preds).astype(int)\n",
    "    )\n",
    "    plt.plot(np.linspace(0, plot_bound, 100), np.linspace(0, plot_bound, 100), \"r--\")\n",
    "\n",
    "    plt.xlabel(\"Actual Rainfall (Test Stations)\")\n",
    "    plt.ylabel(\"Predicted Rainfall (Test Stations)\")\n",
    "    plt.title(\"Test Set Performance\")\n",
    "    plt.grid(True)\n",
    "\n",
    "    # put metrics on the figure (top-left corner)\n",
    "    text_str = f\"Pearson r = {pearson_r_global:.3f}\\nTest RMSE = {final_rmse:.3f}\"\n",
    "    plt.text(\n",
    "        0.05,\n",
    "        0.95,\n",
    "        text_str,\n",
    "        transform=plt.gca().transAxes,\n",
    "        fontsize=12,\n",
    "        verticalalignment=\"top\",\n",
    "        bbox=dict(facecolor=\"white\", alpha=0.7, edgecolor=\"black\"),\n",
    "    )\n",
    "\n",
    "    plt.savefig(f\"experiments/{experiment_name}/test_scatter_plot_{fold}.png\", dpi=300)\n",
    "    plt.close()\n",
    "    print(\n",
    "        f\"Saved test scatter plot to 'experiments/{experiment_name}/test_scatter_plot_{fold}.png'\"\n",
    "    )\n",
    "\n",
    "\n",
    "    collated_station_ids = general_station_ids + rainfall_station_ids\n",
    "    print(station_ids_list)\n",
    "    print(\"COLLATED STAITONS\")\n",
    "    print(collated_station_ids)\n",
    "    for idx in range(predictions_all.shape[1]):\n",
    "        fig, ax = plt.subplots(2,1, figsize=(15,8))\n",
    "        print(\"=====\")\n",
    "        print(idx)\n",
    "        print(station_ids_list[idx])\n",
    "        sid = collated_station_ids[station_ids_list[idx]]\n",
    "        print(f\"SID: {sid}\")\n",
    "        ax[0].plot(predictions_all[:, idx, :], color='blue')\n",
    "        ax[0].set_title(f\"Predicted rainfall {sid}\")\n",
    "        ax[1].plot(truth_all[:, idx, :], color='black')\n",
    "        ax[1].set_title(f\"Actual rainfall {sid}\")\n",
    "        plt.title(f\"{sid}\")\n",
    "        plt.savefig(f\"experiments/{experiment_name}/station_{idx}_preds_actual_plot_{fold}.png\")\n",
    "        # plt.show()\n",
    "    return final_rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ff01a1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils import collate_temporal_graphs_new\n",
    "\n",
    "\n",
    "RMSE = test_model(model, data, device, collate_temporal_graphs_new, fold=0)\n",
    "RMSE1 = test_model(model1, data1, device, collate_temporal_graphs_new, fold=1)\n",
    "# RMSE2 = test_model(model2, data2, device, collate_temporal_graphs_new, fold=2)\n",
    "RMSE3 = test_model(model3, data3, device, collate_temporal_graphs_new, fold=3)\n",
    "RMSE4 = test_model(model4, data4, device, collate_temporal_graphs_new, fold=4)\n",
    "print(f\"TEST RMSE: {RMSE}\")\n",
    "print(f\"TEST RMSE1: {RMSE1}\")\n",
    "# print(f\"TEST RMSE2: {RMSE2}\")\n",
    "print(f\"TEST RMSE3: {RMSE3}\")\n",
    "print(f\"TEST RMSE4: {RMSE4}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f606be2d",
   "metadata": {},
   "source": [
    "# Visualisation of output\n",
    "Test event will be 02-05-2025 0415 to 0615\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb0dc8fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_one_event(test_event_data, radar_features_event, do_plot=True):\n",
    "    \"\"\"\n",
    "    Prepare a single example from `test_event_data` (a pandas slice like\n",
    "    weather_station_df_pivot.iloc[593:602]) and run the model for inference.\n",
    "\n",
    "    This function returns:\n",
    "      gen_out: numpy array of predicted general_station outputs (shape: [num_gen_nodes, out_features])\n",
    "      rain_out: numpy array of predicted rainfall_station outputs (shape: [num_rain_nodes, out_features])\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "\n",
    "    # clone template (so we keep masks/edge_index/order)\n",
    "    test_data = data.clone()\n",
    "\n",
    "    # --- collect station-wise time-series just like you had before ---\n",
    "    test_general_station_data = {}\n",
    "    test_rainfall_station_data = {}\n",
    "\n",
    "    for station in test_event_data.columns.get_level_values(1).unique():\n",
    "        station_cols = (\n",
    "            test_event_data.xs(station, level=1, axis=1)\n",
    "            .interpolate(method=\"linear\")\n",
    "            .fillna(method=\"ffill\")\n",
    "            .fillna(method=\"bfill\")\n",
    "        )\n",
    "        if station in general_station:\n",
    "            test_general_station_data[station] = (\n",
    "                station_cols.values\n",
    "            )  # shape [T, gen_feat]\n",
    "        else:\n",
    "            test_rainfall_station_data[station] = station_cols.values[\n",
    "                :, 0:1\n",
    "            ]  # [T, rain_feat=1]\n",
    "\n",
    "    # Build arrays in the correct node ordering\n",
    "    gen_feats_list = []\n",
    "    rain_feats_list = []\n",
    "\n",
    "    for station in general_station:\n",
    "        gen_feats_list.append(\n",
    "            test_general_station_data[station]\n",
    "        )  # each item: [T, gen_feat_per_t]\n",
    "    for station in rainfall_station:\n",
    "        rain_feats_list.append(\n",
    "            test_rainfall_station_data[station]\n",
    "        )  # each item: [T, rain_feat_per_t]\n",
    "\n",
    "    # Convert to numpy arrays and get shapes\n",
    "    # After np.array(gen_feats_list) => shape [num_gen_nodes, T, gen_feat_per_t]\n",
    "    gen_arr = np.array(gen_feats_list)  # [N_gen, T, Fg]\n",
    "    rain_arr = np.array(rain_feats_list)  # [N_rain, T, Fr]\n",
    "\n",
    "    # --- Convert to the 2-D node-feature format the model expects ---\n",
    "    # There are different sensible choices here:\n",
    "    #  - take last timestep: arr[:, -1, :] -> [N, F]\n",
    "    #  - flatten the time axis into the feature axis: arr.reshape(N, T*F)\n",
    "    # The training/test collate you used produces per-node features (no time dim).\n",
    "    # To match that, we flatten time into features (preserves the whole window).\n",
    "    def flatten_time_axis(arr):\n",
    "        # arr: [N, T, F]\n",
    "        N, T, F = arr.shape\n",
    "        return arr.reshape(N, T * F)  # [N, T*F]\n",
    "\n",
    "    gen_node_feats = gen_arr[:, -1, :].astype(np.float32)\n",
    "    rain_node_feats = rain_arr[:, -1, :].astype(np.float32)  # [N, F]\n",
    "    radar_node_feats = radar_features_event[-1].float()  # [N_radar, F]\n",
    "\n",
    "    # Convert to torch tensors (2-D per node type) and move to device\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    x_dict = {\n",
    "        \"general_station\": torch.tensor(\n",
    "            gen_node_feats, dtype=torch.float, device=device\n",
    "        ),\n",
    "        \"rainfall_station\": torch.tensor(\n",
    "            rain_node_feats, dtype=torch.float, device=device\n",
    "        ),\n",
    "        \"radar_grid\": torch.tensor(radar_node_feats, dtype=torch.float, device=device),\n",
    "    }\n",
    "\n",
    "    # Move edge structures to device (the same ones you used in test_model)\n",
    "    edge_index_dict = {k: v.to(device) for k, v in data.edge_index_dict.items()}\n",
    "    edge_attr_dict = {k: v.to(device) for k, v in data.edge_attr_dict.items()}\n",
    "\n",
    "    # Run model\n",
    "    with torch.no_grad():\n",
    "        out = model(x_dict, edge_index_dict, edge_attr_dict)\n",
    "\n",
    "    # out[...] are torch tensors shaped like [num_nodes, out_features] (depending on your model head)\n",
    "    gen_out = out[\"general_station\"].cpu().numpy()\n",
    "    rain_out = out[\"rainfall_station\"].cpu().numpy()\n",
    "\n",
    "    return gen_out, rain_out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50c852b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_event_data = weather_station_df_pivot.iloc[593:602]  # your 9 timestamps\n",
    "radar_features_event = data[\"radar_grid\"].x[593:602]\n",
    "gen_out, rain_out = visualize_one_event(test_event_data, radar_features_event)\n",
    "out_np = np.concatenate([gen_out, rain_out], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "899f48c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(test_data.edge_index_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ca57f6f",
   "metadata": {},
   "source": [
    "# Visualise rain on radar grid\n",
    "Hard coded to plot only consequitive 9 timestamps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb3f728f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(out_np / 12)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d843bf48",
   "metadata": {},
   "source": [
    "# Visualize Radar Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e460fe47",
   "metadata": {},
   "outputs": [],
   "source": [
    "radar_df = load_radar_dataset(\"sg_radar_data\")\n",
    "\n",
    "visualize_one_radar_image(radar_df=radar_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1993af6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig, ax = plt.subplots(\n",
    "#     3, 3, figsize=(15, 12), subplot_kw={\"projection\": ccrs.PlateCarree()}\n",
    "# )\n",
    "\n",
    "# out_np = out_np / 12\n",
    "# for idx, timestamp in enumerate(out_np):\n",
    "#     output = {}\n",
    "#     count = 0\n",
    "\n",
    "#     for stn in general_station:\n",
    "#         output[stn] = float(timestamp[count])\n",
    "#         count += 1\n",
    "#     for stn in rainfall_station:\n",
    "#         output[stn] = float(timestamp[count])\n",
    "#         count += 1\n",
    "#     axi = ax[idx // 3][idx % 3]\n",
    "#     node_df = pd.Series(output)\n",
    "#     node_df = pandas_to_geodataframe(node_df)\n",
    "#     visualise_gauge_grid(node_df=node_df, ax=axi)\n",
    "#     improved_visualise_radar_grid(\n",
    "#         radar_df.iloc[idx], ax=axi, zoom=bounds_singapore, norm=norm\n",
    "#     )\n",
    "#     visualise_singapore_outline(ax=axi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6af00ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "original_rainfall_rates = (\n",
    "    weather_station_df_pivot.iloc[1773:1797].resample(\"15min\").first()[\"rain_rate\"]\n",
    ")\n",
    "\n",
    "\n",
    "print(original_rainfall_rates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14da652a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a97c8fc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "actual_arr = []\n",
    "pred_arr = []\n",
    "\n",
    "for idx, timestamp in enumerate(out):\n",
    "    output = {}\n",
    "    count = 0\n",
    "    a_arr = []\n",
    "    p_arr = []\n",
    "\n",
    "    for stn in general_station:\n",
    "        output[stn] = float(timestamp[count])\n",
    "        count += 1\n",
    "    for stn in rainfall_station:\n",
    "        output[stn] = float(timestamp[count])\n",
    "        count += 1\n",
    "\n",
    "    for key, value in output.items():\n",
    "        a_arr.append(original_rainfall_rates.iloc[idx][key])\n",
    "        p_arr.append(output[key])\n",
    "    a_arr = list(map(lambda x: float(x), a_arr))\n",
    "    actual_arr.append(a_arr)\n",
    "    pred_arr.append(p_arr)\n",
    "\n",
    "actual_arr = np.array(actual_arr)\n",
    "pred_arr = np.array(pred_arr)\n",
    "\n",
    "print(actual_arr)\n",
    "print(pred_arr)\n",
    "error = []\n",
    "for i in range(len(actual_arr)):\n",
    "    error.append(np.nanmean(actual_arr - pred_arr) ** 2)\n",
    "\n",
    "MSE = np.mean(np.array(error))\n",
    "print(MSE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63ce7ede",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(original_rainfall_rates.iloc[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fyp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
