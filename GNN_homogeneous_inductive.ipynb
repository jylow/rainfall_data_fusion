{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff75aa0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from src.sampling.main import stratified_spatial_kfold_dual\n",
    "\n",
    "import torch\n",
    "from torch_geometric.data import Data\n",
    "from src.raingauge.utils import (\n",
    "    get_station_coordinate_mappings,\n",
    "    load_weather_station_dataset,\n",
    ")\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tqdm\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from scipy.stats import pearsonr\n",
    "from src.radar.utils import load_radar_dataset\n",
    "from src.visualization.radar import (\n",
    "    visualize_one_radar_image,\n",
    ")\n",
    "import matplotlib as mpl\n",
    "from models.gnn import GNNInductive\n",
    "from datetime import datetime\n",
    "from src.performance_logger import PerformanceLogger\n",
    "import os\n",
    "from src.utils import (\n",
    "    build_train_and_full_graph_homogeneous,\n",
    "    add_homogeneous_weather_station_data,\n",
    "    add_homogeneous_mask_to_data,\n",
    "    generate_homogeneous_edges,\n",
    "    add_homogeneous_edge_attributes_to_data,\n",
    "    prepare_homogeneous_inductive_dataset,\n",
    "    debug_dataloader,\n",
    ")\n",
    "import networkx as nx\n",
    "from matplotlib.patches import Patch\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ae2490d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: Geographic extent of Singapore in longitude and latitude\n",
    "bounds_singapore = {\"left\": 103.6, \"right\": 104.1, \"top\": 1.5, \"bottom\": 1.188}\n",
    "bounds = [0.1, 0.2, 0.5, 1, 2, 4, 7, 10, 20]\n",
    "norm = mpl.colors.BoundaryNorm(boundaries=bounds, ncolors=256, extend=\"both\")\n",
    "\n",
    "experiment_name = f\"{datetime.now().strftime('%Y%m%d_%H%M%S')}_new\"\n",
    "os.makedirs(f\"experiments/{experiment_name}\", exist_ok=True)\n",
    "perf = PerformanceLogger(f\"experiments/{experiment_name}/training_log.jsonl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b5f5279",
   "metadata": {},
   "source": [
    "# Preprocess Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4757dbcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = Data()\n",
    "data1 = Data()\n",
    "data2 = Data()\n",
    "data3 = Data()\n",
    "data4 = Data()\n",
    "dtype = torch.float32\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71b30b43",
   "metadata": {},
   "source": [
    "# Preprocess station data.\n",
    "Some stations only contain rainfall information but some stations contain both rainfall and other information.\n",
    "We will split these stations into weather station and general stations \n",
    "\n",
    "Additional info: \n",
    "  Windspeed\n",
    "  Wind Direction\n",
    "  Temperature\n",
    "  Relative Humidity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "173c2616",
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_station_data = load_weather_station_dataset(\"weather_station_data.csv\")\n",
    "weather_station_locations = get_station_coordinate_mappings()\n",
    "print(len(weather_station_locations.keys()))\n",
    "print(len(set(weather_station_data[\"gid\"].values)))\n",
    "cols = list(weather_station_data.columns)\n",
    "cols.remove(\"time_sgt\")\n",
    "cols.remove(\"gid\")\n",
    "\n",
    "weather_station_df_pivot = (\n",
    "    pd.pivot(data=weather_station_data, index=\"time_sgt\", columns=\"gid\", values=cols)\n",
    "    .resample(\"15min\")\n",
    "    .first()\n",
    ")\n",
    "weather_station_df_pivot[\"rain_rate\"] = weather_station_df_pivot[\"rain_rate\"] * 12\n",
    "weather_station_df_counts = weather_station_df_pivot.count().reset_index()\n",
    "\n",
    "weather_station_info = pd.pivot(\n",
    "    data=weather_station_df_counts, index=\"gid\", columns=\"level_0\"\n",
    ")\n",
    "\n",
    "pd.set_option(\"display.max_rows\", None)\n",
    "\n",
    "rainfall_station = [\n",
    "    row[0] for row in weather_station_info.iterrows() if 0 in row[1].value_counts()\n",
    "]\n",
    "general_station = [s for s in weather_station_locations if s not in rainfall_station]\n",
    "\n",
    "print(rainfall_station)\n",
    "print(general_station)\n",
    "count = 0\n",
    "for row in weather_station_df_pivot[\"rain_rate\"].iterrows():\n",
    "    if np.nansum(row[1].to_numpy()) != 0:\n",
    "        count += 1\n",
    "print(f\"Number of timesteps that contain rain: {count}\")\n",
    "print(f\"Total_timesteps = {weather_station_df_pivot.shape[0]}\")\n",
    "\n",
    "# After loading weather_station_df_pivot\n",
    "print(\"--- Station Data Stats ---\")\n",
    "print(weather_station_df_pivot.describe())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1e91f6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "general_station_data = {}\n",
    "rainfall_station_data = {}\n",
    "\n",
    "# TODO: Temporal Data Leakage - Filling missing values in the training set using all data including validation or test set is wrong.\n",
    "# Extract and interpolate station data\n",
    "for station in weather_station_df_pivot.columns.get_level_values(1).unique():\n",
    "    station_cols = (\n",
    "        weather_station_df_pivot.xs(station, level=1, axis=1)\n",
    "        .interpolate(method=\"linear\")\n",
    "        .fillna(method=\"ffill\")\n",
    "        .fillna(method=\"bfill\")\n",
    "    )\n",
    "    if station in general_station:\n",
    "        general_station_data[station] = station_cols.values\n",
    "    else:\n",
    "        rainfall_station_data[station] = station_cols.values[:, 0:1]\n",
    "\n",
    "general_station_temp = [stn for stn in general_station if stn != \"S108\"]\n",
    "general_station = general_station_temp\n",
    "\n",
    "# Prepare features in the correct order\n",
    "general_station_features = []\n",
    "rainfall_station_features = []\n",
    "general_station_ids = []\n",
    "rainfall_station_ids = []\n",
    "\n",
    "for station in general_station:\n",
    "    station_feat = general_station_data[station]\n",
    "    general_station_features.append(station_feat)\n",
    "    general_station_ids.append(station)\n",
    "\n",
    "for station in rainfall_station:\n",
    "    station_feat = rainfall_station_data[station]\n",
    "    rainfall_station_features.append(station_feat)\n",
    "    rainfall_station_ids.append(station)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59c1da65",
   "metadata": {},
   "source": [
    "# Add Station Features to HeteroData Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26124db1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add station features to HeteroData\n",
    "\n",
    "include_metastation_info = False\n",
    "data = add_homogeneous_weather_station_data(\n",
    "    data,\n",
    "    general_station_features,\n",
    "    rainfall_station_features,\n",
    "    general_station_ids,\n",
    "    rainfall_station_ids,\n",
    "    dtype=dtype,\n",
    ")\n",
    "data1 = add_homogeneous_weather_station_data(\n",
    "    data1,\n",
    "    general_station_features,\n",
    "    rainfall_station_features,\n",
    "    general_station_ids,\n",
    "    rainfall_station_ids,\n",
    "    dtype=dtype,\n",
    ")\n",
    "data2 = add_homogeneous_weather_station_data(\n",
    "    data2,\n",
    "    general_station_features,\n",
    "    rainfall_station_features,\n",
    "    general_station_ids,\n",
    "    rainfall_station_ids,\n",
    "    dtype=dtype,\n",
    ")\n",
    "data3 = add_homogeneous_weather_station_data(\n",
    "    data3,\n",
    "    general_station_features,\n",
    "    rainfall_station_features,\n",
    "    general_station_ids,\n",
    "    rainfall_station_ids,\n",
    "    dtype=dtype,\n",
    ")\n",
    "data4 = add_homogeneous_weather_station_data(\n",
    "    data4,\n",
    "    general_station_features,\n",
    "    rainfall_station_features,\n",
    "    general_station_ids,\n",
    "    rainfall_station_ids,\n",
    "    dtype=dtype,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b62d6977",
   "metadata": {},
   "source": [
    "# Stratified K Fold Spatial Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e7038fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "split_info = stratified_spatial_kfold_dual(\n",
    "    weather_station_locations, seed=123, plot=True\n",
    ")\n",
    "print(split_info)\n",
    "stations = general_station + rainfall_station\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82465f39",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = add_homogeneous_mask_to_data(data, split_info[0], stations)\n",
    "data1 = add_homogeneous_mask_to_data(data1, split_info[1], stations)\n",
    "data2 = add_homogeneous_mask_to_data(data2, split_info[2], stations)\n",
    "data3 = add_homogeneous_mask_to_data(data3, split_info[3], stations)\n",
    "data4 = add_homogeneous_mask_to_data(data4, split_info[4], stations)\n",
    "\n",
    "print(\"Data: \\n\", data)\n",
    "print(\"Data1: \\n\", data1)\n",
    "print(\"Data2: \\n\", data2)\n",
    "print(\"Data3: \\n\", data3)\n",
    "print(\"Data4: \\n\", data4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3ae33c3",
   "metadata": {},
   "source": [
    "# Edge generation\n",
    "We consider the location of the stations when performing our edge generation. \n",
    "General station locations and rainfall station locations will be considered the same and we will make a connection across the nodes if required. This will ensure that we can connect both the layers together in the graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f5b7b66",
   "metadata": {},
   "outputs": [],
   "source": [
    "K = 4  # Number of neighbors per node\n",
    "edges, edge_attributes = generate_homogeneous_edges(\n",
    "    weather_station_locations,\n",
    "    stations=stations,\n",
    "    K=K,\n",
    ")\n",
    "\n",
    "data = add_homogeneous_edge_attributes_to_data(\n",
    "    data, edges, edge_attributes, dtype=dtype\n",
    ")\n",
    "data1 = add_homogeneous_edge_attributes_to_data(\n",
    "    data1, edges, edge_attributes, dtype=dtype\n",
    ")\n",
    "data2 = add_homogeneous_edge_attributes_to_data(\n",
    "    data2, edges, edge_attributes, dtype=dtype\n",
    ")\n",
    "data3 = add_homogeneous_edge_attributes_to_data(\n",
    "    data3, edges, edge_attributes, dtype=dtype\n",
    ")\n",
    "data4 = add_homogeneous_edge_attributes_to_data(\n",
    "    data4, edges, edge_attributes, dtype=dtype\n",
    ")\n",
    "\n",
    "print(data)\n",
    "print(data1)\n",
    "print(data2)\n",
    "print(data3)\n",
    "print(data4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14b1466f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# FINAL SUMMARY\n",
    "# ============================================================================\n",
    "# print_data_structure(data)\n",
    "# print_data_structure(data1)\n",
    "# print_data_structure(data2)\n",
    "# print_data_structure(data3)\n",
    "# print_data_structure(data4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e41d2a99",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_graphs = []\n",
    "validation_graphs = []\n",
    "full_graphs = []\n",
    "\n",
    "for fold_idx in range(5):\n",
    "    train_g, val_g, full_g = build_train_and_full_graph_homogeneous(\n",
    "        data, split_info[fold_idx], stations\n",
    "    )\n",
    "    train_graphs.append(train_g)\n",
    "    validation_graphs.append(val_g)\n",
    "    full_graphs.append(full_g)\n",
    "\n",
    "print(\n",
    "    f\"Train Data: {train_graphs[0]}\\nValidation Data: {validation_graphs[0]}\\nData Full: {full_graphs[0]}\"\n",
    ")\n",
    "\n",
    "full_graph = full_graphs[0]\n",
    "validation_graph = validation_graphs[0]\n",
    "train_graph = train_graphs[0]\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"GRAPH STRUCTURE ANALYSIS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# ============================================\n",
    "# FULL GRAPH SETUP\n",
    "# ============================================\n",
    "print(\"\\n--- Full Graph ---\")\n",
    "station_count = full_graph.station_id.shape[0]  # 63 stations\n",
    "print(f\"Total stations: {station_count}\")\n",
    "print(f\"Train nodes: {full_graph.train_mask.sum().item()}\")\n",
    "print(f\"Val nodes: {full_graph.val_mask.sum().item()}\")\n",
    "print(f\"Test nodes: {full_graph.test_mask.sum().item()}\")\n",
    "\n",
    "# Create NetworkX graph for full graph\n",
    "G_full = nx.Graph()\n",
    "G_full.add_nodes_from(range(station_count))\n",
    "\n",
    "# Add edges (these are already in global node indices)\n",
    "edges_full = full_graph.edge_index.numpy().T\n",
    "G_full.add_edges_from(edges_full)\n",
    "\n",
    "print(f\"Full graph edges: {G_full.number_of_edges()}\")\n",
    "\n",
    "# ============================================\n",
    "# TRAIN GRAPH SETUP (CORRECTED)\n",
    "# ============================================\n",
    "print(\"\\n--- Train Graph ---\")\n",
    "\n",
    "# CRITICAL: train_graph uses LOCAL indices (0 to num_train_nodes-1)\n",
    "# but train_graph.orig_id maps back to GLOBAL indices\n",
    "num_train_nodes = train_graph.x.shape[1]\n",
    "print(f\"Train graph nodes: {num_train_nodes}\")\n",
    "\n",
    "# Get mapping: local_idx -> global_idx\n",
    "if hasattr(train_graph, \"orig_id\"):\n",
    "    orig_ids = train_graph.orig_id.numpy()\n",
    "    print(f\"orig_id mapping exists: {len(orig_ids)} mappings\")\n",
    "else:\n",
    "    print(\"ERROR: train_graph missing 'orig_id' attribute!\")\n",
    "    # Fallback: assume train nodes are the first ones with train_mask=True in full graph\n",
    "    orig_ids = np.where(full_graph.train_mask.numpy() | full_graph.val_mask.numpy())[0]\n",
    "    print(f\"Reconstructed orig_id from masks: {len(orig_ids)} mappings\")\n",
    "\n",
    "# Create reverse mapping: global_idx -> local_idx\n",
    "global_to_local = {int(g): i for i, g in enumerate(orig_ids)}\n",
    "\n",
    "print(f\"Train graph local indices: 0 to {num_train_nodes - 1}\")\n",
    "print(f\"Train graph global indices: {orig_ids[:5]}... (first 5)\")\n",
    "\n",
    "# Create NetworkX graph for train graph\n",
    "G_train = nx.Graph()\n",
    "G_train.add_nodes_from(range(num_train_nodes))\n",
    "\n",
    "# Add edges (train_graph.edge_index uses LOCAL indices)\n",
    "edges_train_local = train_graph.edge_index.numpy().T\n",
    "G_train.add_edges_from(edges_train_local)\n",
    "\n",
    "print(f\"Train graph edges: {G_train.number_of_edges()}\")\n",
    "\n",
    "# ============================================\n",
    "# VALIDATION GRAPH (train + val nodes, re-colored)\n",
    "# ============================================\n",
    "print(\"\\n--- Validation Graph (train + val nodes) ---\")\n",
    "\n",
    "# The validation graph uses the train+val node\n",
    "num_valgraph_nodes = validation_graph.x.shape[1]  # identical to train graph\n",
    "print(f\"Validation graph nodes: {num_valgraph_nodes}\")\n",
    "\n",
    "# Get mapping: local_idx -> global_idx\n",
    "if hasattr(validation_graph, \"orig_id\"):\n",
    "    val_orig_ids = validation_graph.orig_id.numpy()\n",
    "    print(f\"orig_id mapping exists: {len(val_orig_ids)} mappings\")\n",
    "else:\n",
    "    print(\"ERROR: train_graph missing 'orig_id' attribute!\")\n",
    "    # Fallback: assume train nodes are the first ones with train_mask=True in full graph\n",
    "    val_orig_ids = np.where(full_graph.train_mask.numpy())[0]\n",
    "    print(f\"Reconstructed orig_id from masks: {len(val_orig_ids)} mappings\")\n",
    "\n",
    "# Create NetworkX graph\n",
    "G_valgraph = nx.Graph()\n",
    "G_valgraph.add_nodes_from(range(num_valgraph_nodes))\n",
    "\n",
    "# Edges are the train+val only\n",
    "edges_val_local = validation_graph.edge_index.numpy().T\n",
    "G_valgraph.add_edges_from(edges_val_local)\n",
    "\n",
    "print(f\"Validation graph edges: {G_valgraph.number_of_edges()}\")\n",
    "\n",
    "# ============================================\n",
    "# GENERATE GEOGRAPHICAL LAYOUT\n",
    "# ============================================\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"GENERATING GEOGRAPHICAL POSITIONS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# --- Full Graph Positions ---\n",
    "# Key: node_idx (0 to 62) -> (lon, lat)\n",
    "pos_full = {}\n",
    "for node_idx in range(station_count):\n",
    "    station_str_id = stations[full_graph.station_id[node_idx].item()]\n",
    "    if station_str_id in weather_station_locations:\n",
    "        lat, lon = weather_station_locations[station_str_id]\n",
    "        pos_full[node_idx] = (lon, lat)  # NetworkX uses (x, y) = (lon, lat)\n",
    "    else:\n",
    "        print(f\"WARNING: Station {station_str_id} not found in locations\")\n",
    "        pos_full[node_idx] = (0, 0)  # Default position\n",
    "\n",
    "print(f\"Full graph positions generated: {len(pos_full)}\")\n",
    "\n",
    "# --- Train Graph Positions (CORRECTED) ---\n",
    "# Key: local_idx (0 to 54) -> (lon, lat)\n",
    "# Use orig_id to map back to global station indices\n",
    "pos_train = {}\n",
    "for local_idx in range(num_train_nodes):\n",
    "    global_idx = int(orig_ids[local_idx])\n",
    "\n",
    "    # Get station string ID from full graph\n",
    "    station_str_id = stations[full_graph.station_id[global_idx].item()]\n",
    "\n",
    "    if station_str_id in weather_station_locations:\n",
    "        lat, lon = weather_station_locations[station_str_id]\n",
    "        pos_train[local_idx] = (lon, lat)\n",
    "    else:\n",
    "        print(f\"WARNING: Station {station_str_id} not found in locations\")\n",
    "        pos_train[local_idx] = (0, 0)\n",
    "\n",
    "print(f\"Train graph positions generated: {len(pos_train)}\")\n",
    "\n",
    "# --- Validation Graph Pos (same positions as train graph)\n",
    "pos_valgraph = {}\n",
    "for local_idx in range(num_valgraph_nodes):\n",
    "    global_idx = int(val_orig_ids[local_idx])\n",
    "    station_str_id = stations[full_graph.station_id[global_idx].item()]\n",
    "\n",
    "    if station_str_id in weather_station_locations:\n",
    "        lat, lon = weather_station_locations[station_str_id]\n",
    "        pos_valgraph[local_idx] = (lon, lat)\n",
    "    else:\n",
    "        pos_valgraph[local_idx] = (0, 0)\n",
    "\n",
    "print(f\"Validation graph positions generated: {len(pos_valgraph)}\")\n",
    "# ============================================\n",
    "# CREATE CONSISTENT COLOR MAPS\n",
    "# ============================================\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"CREATING COLOR MAPS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# --- Full Graph Colors ---\n",
    "color_map_full = []\n",
    "node_labels_full = {}\n",
    "\n",
    "for node_idx in range(station_count):\n",
    "    if full_graph.train_mask[node_idx]:\n",
    "        color_map_full.append(\"green\")\n",
    "        label = \"Train\"\n",
    "    elif full_graph.val_mask[node_idx]:\n",
    "        color_map_full.append(\"blue\")\n",
    "        label = \"Val\"\n",
    "    elif full_graph.test_mask[node_idx]:\n",
    "        color_map_full.append(\"red\")\n",
    "        label = \"Test\"\n",
    "    else:\n",
    "        color_map_full.append(\"gray\")\n",
    "        label = \"Unknown\"\n",
    "\n",
    "    # Optional: add node labels showing global index\n",
    "    node_labels_full[node_idx] = f\"{node_idx}\"\n",
    "\n",
    "print(\n",
    "    f\"Full graph - Train: {color_map_full.count('green')}, \"\n",
    "    f\"Val: {color_map_full.count('blue')}, \"\n",
    "    f\"Test: {color_map_full.count('red')}\"\n",
    ")\n",
    "\n",
    "# --- Train Graph Colors (CORRECTED) ---\n",
    "# Must use train_graph masks directly (already in local indices)\n",
    "color_map_train = []\n",
    "node_labels_train = {}\n",
    "\n",
    "for local_idx in range(num_train_nodes):\n",
    "    # Use train_graph masks (these are in local indices)\n",
    "    if train_graph.train_mask[local_idx]:\n",
    "        color_map_train.append(\"green\")\n",
    "        label = \"Train\"\n",
    "    elif train_graph.val_mask[local_idx]:\n",
    "        color_map_train.append(\"blue\")\n",
    "        label = \"Val\"\n",
    "    elif (\n",
    "        train_graph.test_mask[local_idx] if hasattr(train_graph, \"test_mask\") else False\n",
    "    ):\n",
    "        color_map_train.append(\"red\")\n",
    "        label = \"Test\"\n",
    "    else:\n",
    "        color_map_train.append(\"gray\")\n",
    "        label = \"Unknown\"\n",
    "\n",
    "    # Show both local and global indices\n",
    "    global_idx = int(orig_ids[local_idx])\n",
    "    node_labels_train[local_idx] = f\"{local_idx}\\n({global_idx})\"\n",
    "\n",
    "print(\n",
    "    f\"Train graph - Train: {color_map_train.count('green')}, \"\n",
    "    f\"Val: {color_map_train.count('blue')}, \"\n",
    "    f\"Test: {color_map_train.count('red')}\"\n",
    ")\n",
    "\n",
    "# --- Validation Graph Colors\n",
    "color_map_val = []\n",
    "node_labels_val = {}\n",
    "\n",
    "for local_idx in range(num_valgraph_nodes):\n",
    "    if validation_graph.train_mask[local_idx]:\n",
    "        color_map_val.append(\"green\")\n",
    "        node_type = \"Train\"\n",
    "    elif validation_graph.val_mask[local_idx]:\n",
    "        color_map_val.append(\"blue\")\n",
    "        node_type = \"Val\"\n",
    "    else:\n",
    "        color_map_val.append(\"gray\")\n",
    "        node_type = \"Other\"\n",
    "\n",
    "    global_idx = int(val_orig_ids[local_idx])\n",
    "    node_labels_val[local_idx] = f\"{local_idx}\\n({global_idx})\"\n",
    "\n",
    "print(\n",
    "    f\"Validation graph - Train: {color_map_val.count('green')}, \"\n",
    "    f\"Val: {color_map_val.count('blue')}, \"\n",
    "    f\"Test: {color_map_val.count('red')}\"\n",
    ")\n",
    "\n",
    "# ============================================\n",
    "# VERIFICATION: Check Consistency\n",
    "# ============================================\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"CONSISTENCY VERIFICATION\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Check: Train nodes in train_graph should match train/val nodes in full_graph\n",
    "train_val_in_full = set(\n",
    "    np.where((full_graph.train_mask | full_graph.val_mask).numpy())[0]\n",
    ")\n",
    "train_nodes_mapped = set(orig_ids)\n",
    "\n",
    "if train_val_in_full == train_nodes_mapped:\n",
    "    print(\"✅ Node sets are consistent!\")\n",
    "else:\n",
    "    print(\"⚠️ WARNING: Node sets don't match!\")\n",
    "    print(f\"   Full graph train+val: {len(train_val_in_full)} nodes\")\n",
    "    print(f\"   Train graph (via orig_id): {len(train_nodes_mapped)} nodes\")\n",
    "    print(f\"   Difference: {train_val_in_full - train_nodes_mapped}\")\n",
    "\n",
    "# Check: Color distribution should match\n",
    "full_train_count = sum(1 for i in orig_ids if full_graph.train_mask[i])\n",
    "train_train_count = color_map_train.count(\"green\")\n",
    "\n",
    "print(\"\\nTrain node count:\")\n",
    "print(f\"   Full graph (for train graph nodes): {full_train_count}\")\n",
    "print(f\"   Train graph: {train_train_count}\")\n",
    "print(f\"   Match: {'✅' if full_train_count == train_train_count else '❌'}\")\n",
    "\n",
    "# ============================================\n",
    "# DRAW THE PLOTS\n",
    "# ============================================\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"DRAWING GRAPHS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(30, 10))\n",
    "\n",
    "# --- Plot 1: Full Graph ---\n",
    "ax = axes[0]\n",
    "nx.draw(\n",
    "    G_full,\n",
    "    pos_full,\n",
    "    node_color=color_map_full,\n",
    "    with_labels=True,\n",
    "    labels=node_labels_full,\n",
    "    node_size=400,\n",
    "    font_size=7,\n",
    "    font_weight=\"bold\",\n",
    "    edge_color=\"gray\",\n",
    "    width=1.5,\n",
    "    ax=ax,\n",
    ")\n",
    "ax.set_title(\n",
    "    f\"Full Graph ({station_count} stations)\\n\"\n",
    "    f\"Train={color_map_full.count('green')}, \"\n",
    "    f\"Val={color_map_full.count('blue')}, \"\n",
    "    f\"Test={color_map_full.count('red')}\",\n",
    "    fontsize=14,\n",
    "    fontweight=\"bold\",\n",
    ")\n",
    "\n",
    "# --- Plot 2: Train Graph ---\n",
    "ax = axes[1]\n",
    "nx.draw(\n",
    "    G_train,\n",
    "    pos_train,\n",
    "    node_color=color_map_train,\n",
    "    with_labels=True,\n",
    "    labels=node_labels_train,\n",
    "    node_size=400,\n",
    "    font_size=7,\n",
    "    font_weight=\"bold\",\n",
    "    edge_color=\"gray\",\n",
    "    width=1.5,\n",
    "    ax=ax,\n",
    ")\n",
    "ax.set_title(\n",
    "    f\"Train Graph ({num_train_nodes} stations)\\n\"\n",
    "    f\"Local(Global) indices shown\\n\"\n",
    "    f\"Train={color_map_train.count('green')}, \"\n",
    "    f\"Val={color_map_train.count('blue')}\",\n",
    "    fontsize=14,\n",
    "    fontweight=\"bold\",\n",
    ")\n",
    "\n",
    "# --- Plot 3: Validation Graph (train + val nodes) ---\n",
    "ax = axes[2]\n",
    "nx.draw(\n",
    "    G_valgraph,\n",
    "    pos_valgraph,\n",
    "    node_color=color_map_val,\n",
    "    with_labels=True,\n",
    "    labels=node_labels_val,\n",
    "    node_size=400,\n",
    "    font_size=7,\n",
    "    font_weight=\"bold\",\n",
    "    edge_color=\"gray\",\n",
    "    width=1.5,\n",
    "    ax=ax,\n",
    ")\n",
    "ax.set_title(\"Validation Graph (Train + Val Nodes)\", fontsize=14, fontweight=\"bold\")\n",
    "\n",
    "# Add legend\n",
    "legend_elements = [\n",
    "    Patch(facecolor=\"green\", label=\"Train\"),\n",
    "    Patch(facecolor=\"blue\", label=\"Validation\"),\n",
    "    Patch(facecolor=\"red\", label=\"Test\"),\n",
    "    Patch(facecolor=\"gray\", label=\"Unknown\"),\n",
    "]\n",
    "fig.legend(handles=legend_elements, loc=\"upper center\", ncol=4, fontsize=12)\n",
    "\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
    "plt.savefig(\"graph_comparison_corrected.png\", dpi=150, bbox_inches=\"tight\")\n",
    "print(\"✅ Graphs saved to 'graph_comparison_corrected.png'\")\n",
    "plt.show()\n",
    "\n",
    "# ============================================\n",
    "# DETAILED MAPPING TABLE (for debugging)\n",
    "# ============================================\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"NODE MAPPING TABLE (First 10 nodes)\")\n",
    "print(\"=\" * 70)\n",
    "print(\n",
    "    f\"{'Local Idx':<12} {'Global Idx':<12} {'Station ID':<15} {'Type in Train':<15} {'Type in Full':<15}\"\n",
    ")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "for local_idx in range(min(10, num_train_nodes)):\n",
    "    global_idx = int(orig_ids[local_idx])\n",
    "    station_str_id = stations[full_graph.station_id[global_idx].item()]\n",
    "\n",
    "    # Type in train graph\n",
    "    if train_graph.train_mask[local_idx]:\n",
    "        type_train = \"Train\"\n",
    "    elif train_graph.val_mask[local_idx]:\n",
    "        type_train = \"Val\"\n",
    "    else:\n",
    "        type_train = \"Other\"\n",
    "\n",
    "    # Type in full graph\n",
    "    if full_graph.train_mask[global_idx]:\n",
    "        type_full = \"Train\"\n",
    "    elif full_graph.val_mask[global_idx]:\n",
    "        type_full = \"Val\"\n",
    "    elif full_graph.test_mask[global_idx]:\n",
    "        type_full = \"Test\"\n",
    "    else:\n",
    "        type_full = \"Other\"\n",
    "\n",
    "    print(\n",
    "        f\"{local_idx:<12} {global_idx:<12} {station_str_id:<15} {type_train:<15} {type_full:<15}\"\n",
    "    )\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"SUMMARY\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"\"\"\n",
    "✅ Full Graph: {station_count} nodes, {G_full.number_of_edges()} edges\n",
    "   - Uses GLOBAL indices (0 to {station_count - 1})\n",
    "   - Shows Train/Val/Test splits\n",
    "   \n",
    "✅ Train Graph: {num_train_nodes} nodes, {G_train.number_of_edges()} edges  \n",
    "   - Uses LOCAL indices (0 to {num_train_nodes - 1})\n",
    "   - Contains only Train nodes from full graph\n",
    "   - Labels show: Local(Global) index mapping\n",
    "\n",
    "✅ Train Graph: {num_valgraph_nodes} nodes, {G_valgraph.number_of_edges()} edges  \n",
    "   - Uses LOCAL indices (0 to {num_valgraph_nodes - 1})\n",
    "   - Contains only Train+Val nodes from full graph\n",
    "   - Labels show: Local(Global) index mapping\n",
    "   \n",
    "✅ Consistency: Node colors and positions are now aligned!\n",
    "   - Both graphs use the same geographical layout\n",
    "   - Color coding matches across both visualizations\n",
    "   - orig_id properly maps local -> global indices\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f959a81",
   "metadata": {},
   "source": [
    "# Creating the GNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "131359f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_channels = 4\n",
    "in_channels = 1\n",
    "out_channels = 1\n",
    "num_layers = 8\n",
    "\n",
    "model = GNNInductive(\n",
    "    in_channels=in_channels,\n",
    "    hidden_channels=hidden_channels,\n",
    "    out_channels=out_channels,\n",
    "    num_layers=num_layers,\n",
    ")\n",
    "model1 = GNNInductive(\n",
    "    in_channels=in_channels,\n",
    "    hidden_channels=hidden_channels,\n",
    "    out_channels=out_channels,\n",
    "    num_layers=num_layers,\n",
    ")\n",
    "model2 = GNNInductive(\n",
    "    in_channels=in_channels,\n",
    "    hidden_channels=hidden_channels,\n",
    "    out_channels=out_channels,\n",
    "    num_layers=num_layers,\n",
    ")\n",
    "model3 = GNNInductive(\n",
    "    in_channels=in_channels,\n",
    "    hidden_channels=hidden_channels,\n",
    "    out_channels=out_channels,\n",
    "    num_layers=num_layers,\n",
    ")\n",
    "model4 = GNNInductive(\n",
    "    in_channels=in_channels,\n",
    "    hidden_channels=hidden_channels,\n",
    "    out_channels=out_channels,\n",
    "    num_layers=num_layers,\n",
    ")\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model.to(device=device)\n",
    "model1.to(device=device)\n",
    "model2.to(device=device)\n",
    "model3.to(device=device)\n",
    "model4.to(device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c5f28c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(\n",
    "    model,\n",
    "    dataloader,\n",
    "    optimizer,\n",
    "    device,\n",
    "    verbose=False,\n",
    "    log_file=\"training_gnn_new_debug.log\",\n",
    "    random_noise_masking=False,\n",
    "    scheduler=None,\n",
    "):\n",
    "    \"\"\"\n",
    "    Corrected training loop with gradient debugging.\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    epoch_losses = []\n",
    "    charge_bar = tqdm.tqdm(dataloader, desc=\"training\")\n",
    "\n",
    "    for batch_idx, batch in enumerate(charge_bar):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # PyG Batch object - move to device\n",
    "        batch = batch.to(device)\n",
    "\n",
    "        # Extract from PyG Batch format\n",
    "        x = batch.x  # [B*N, F]\n",
    "        y = batch.y  # [B*N, Tgt]\n",
    "        mask = batch.mask  # [N] - PROBLEM: single mask for one graph\n",
    "        edge_index = batch.edge_index\n",
    "        edge_attr = batch.edge_attr if batch.edge_attr is not None else None\n",
    "        num_graphs = batch.num_graphs\n",
    "\n",
    "        # Properly replicate mask across batch\n",
    "        # If mask is [N] and we have B graphs with N nodes each, replicate it\n",
    "        if mask.shape[0] == x.shape[0] // num_graphs:\n",
    "            # mask is per-graph, replicate for batch\n",
    "            mask_expanded = mask.repeat(num_graphs)  # [B*N]\n",
    "        else:\n",
    "            mask_expanded = mask  # Already expanded\n",
    "\n",
    "        assert mask_expanded.shape[0] == x.shape[0], (\n",
    "            f\"mask_expanded {mask_expanded.shape} != x {x.shape}\"\n",
    "        )\n",
    "\n",
    "        # Check if any trainable nodes exist\n",
    "        num_trainable = mask_expanded.sum().item()\n",
    "        if num_trainable == 0:\n",
    "            print(f\"WARNING: No trainable nodes in batch {batch_idx}!\")\n",
    "            continue\n",
    "\n",
    "        # Optionally add noise\n",
    "        if random_noise_masking:\n",
    "            noise = torch.randn_like(x) * 0.1\n",
    "            x = x + noise\n",
    "\n",
    "        # Mask features properly\n",
    "        x_masked = x.clone()\n",
    "        x_masked[~mask_expanded] = 0.0\n",
    "\n",
    "        # Verify gradients can flow\n",
    "        x_masked.requires_grad_(True)\n",
    "\n",
    "        # Forward pass\n",
    "        out = model(x_masked, edge_index, edge_attributes=edge_attr)\n",
    "\n",
    "        # Compute loss ONLY on trainable nodes\n",
    "        loss = F.mse_loss(out[mask_expanded], y[mask_expanded])\n",
    "\n",
    "        if verbose or batch_idx == 0:\n",
    "            print(f\"loss: {loss.item()}, requires_grad: {loss.requires_grad}\")\n",
    "\n",
    "        # Check gradients before backward\n",
    "        loss.backward()\n",
    "\n",
    "        # Check if any gradients were computed\n",
    "        total_grad_norm = 0.0\n",
    "        num_params_with_grad = 0\n",
    "        for name, param in model.named_parameters():\n",
    "            if param.grad is not None:\n",
    "                grad_norm = param.grad.norm().item()\n",
    "                total_grad_norm += grad_norm**2\n",
    "                num_params_with_grad += 1\n",
    "                if verbose and grad_norm > 1e-6:\n",
    "                    print(f\"  {name}: grad_norm={grad_norm:.2e}\")\n",
    "            elif verbose or batch_idx == 0:\n",
    "                print(f\"  {name}: NO GRADIENT\")\n",
    "\n",
    "        total_grad_norm = np.sqrt(total_grad_norm)\n",
    "\n",
    "        if num_params_with_grad == 0:\n",
    "            print(f\"ERROR: No gradients computed in batch {batch_idx}!\")\n",
    "            return None\n",
    "\n",
    "        if total_grad_norm < 1e-8 and batch_idx % 20 == 0:\n",
    "            print(\n",
    "                f\"WARNING: Very small gradient norm {total_grad_norm:.2e} in batch {batch_idx}\"\n",
    "            )\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        if scheduler is not None:\n",
    "            scheduler.step()\n",
    "\n",
    "        epoch_losses.append(loss.item())\n",
    "        charge_bar.set_postfix(\n",
    "            {\n",
    "                \"loss\": loss.item(),\n",
    "                \"grad_norm\": total_grad_norm,\n",
    "                \"trainable\": num_trainable,\n",
    "            }\n",
    "        )\n",
    "\n",
    "    return float(np.mean(epoch_losses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c26809b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(\n",
    "    model, dataloader, device, verbose=False, log_file=\"validation_gnn_new_debug.log\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Validation loop for PyG batched graph data (inductive setting).\n",
    "\n",
    "    Key aspects:\n",
    "    1. Data comes as PyG Batch objects\n",
    "    2. Features are [B*N, F], already batched and flattened\n",
    "    3. Mask is [N] - single mask for one graph, replicated across batch\n",
    "    4. Computes metrics ONLY on validation nodes (where mask=True)\n",
    "    5. No gradients computed - eval mode\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    epoch_losses = []\n",
    "    all_preds = []\n",
    "    all_targets = []\n",
    "\n",
    "    charge_bar = tqdm.tqdm(dataloader, desc=\"validation\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in charge_bar:\n",
    "            # PyG Batch object - move to device\n",
    "            batch = batch.to(device)\n",
    "\n",
    "            # Extract from PyG Batch format\n",
    "            x = batch.x  # [B*N, F] - already batched and flattened\n",
    "            y = batch.y  # [B*N, Tgt] - already batched and flattened\n",
    "            mask = batch.mask  # [N] - single mask for one graph\n",
    "            edge_index = batch.edge_index  # [2, E*B] - offset edge indices\n",
    "            edge_attr = batch.edge_attr if batch.edge_attr is not None else None\n",
    "\n",
    "            assert mask.shape[0] == x.shape[0], (\n",
    "                f\"mask size {mask.shape[0]} != x size {x.shape[0]}\"\n",
    "            )\n",
    "\n",
    "            # Forward pass\n",
    "            out = model(x, edge_index, edge_attributes=edge_attr)  # [B*N, out_channels]\n",
    "\n",
    "            # Compute loss ONLY on validation nodes\n",
    "            val_mask = mask  # [B*N] boolean mask\n",
    "\n",
    "            loss = F.mse_loss(out[val_mask], y[val_mask])\n",
    "            epoch_losses.append(loss.item())\n",
    "\n",
    "            # Store predictions and targets for metric computation\n",
    "            all_preds.append(out[val_mask].detach().cpu())\n",
    "            all_targets.append(y[val_mask].detach().cpu())\n",
    "\n",
    "            charge_bar.set_postfix({\"loss\": loss.item()})\n",
    "\n",
    "    # Concatenate all predictions and targets\n",
    "    all_preds = torch.cat(all_preds, dim=0)  # [Total_val_nodes, out_channels]\n",
    "    all_targets = torch.cat(all_targets, dim=0)  # [Total_val_nodes, out_channels]\n",
    "\n",
    "    # Compute metrics\n",
    "    mean_loss = float(np.mean(epoch_losses))\n",
    "\n",
    "    return mean_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c428c47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set seeds\n",
    "\n",
    "seed = 123\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "perf.log_model_config(model.config)\n",
    "\n",
    "batch_size = 16\n",
    "train_loader, val_loader = prepare_homogeneous_inductive_dataset(\n",
    "    train_graphs[0], validation_graphs[0], batch_size=batch_size, mode=\"train\"\n",
    ")\n",
    "train_loader1, val_loader1 = prepare_homogeneous_inductive_dataset(\n",
    "    train_graphs[1], validation_graphs[1], batch_size=batch_size, mode=\"train\"\n",
    ")\n",
    "train_loader2, val_loader2 = prepare_homogeneous_inductive_dataset(\n",
    "    train_graphs[2], validation_graphs[2], batch_size=batch_size, mode=\"train\"\n",
    ")\n",
    "train_loader3, val_loader3 = prepare_homogeneous_inductive_dataset(\n",
    "    train_graphs[3], validation_graphs[3], batch_size=batch_size, mode=\"train\"\n",
    ")\n",
    "train_loader4, val_loader4 = prepare_homogeneous_inductive_dataset(\n",
    "    train_graphs[4], validation_graphs[4], batch_size=batch_size, mode=\"train\"\n",
    ")\n",
    "\n",
    "debug_dataloader(train_loader)\n",
    "debug_dataloader(val_loader)\n",
    "\n",
    "\n",
    "def train(model, train_loader, val_loader, fold, device=\"cpu\"):\n",
    "    # CHECK 1: Print initial weights\n",
    "    first_param = next(model.parameters())\n",
    "    print(f\"Initial weight sample: {first_param.data.flatten()[:5]}\")\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.005)\n",
    "    training_loss_arr = []\n",
    "    validation_loss_arr = []\n",
    "    early = 0\n",
    "    mini = 1000\n",
    "    stopping_condition = 5\n",
    "    epochs = 0\n",
    "    total_epochs = 10\n",
    "    print(f\"-----FOLD: {fold}-----\")\n",
    "    training_start = time.time()\n",
    "    for i in range(total_epochs):\n",
    "        print(f\"-----EPOCH: {i + 1}-----\")\n",
    "\n",
    "        # CHECK 2: Print weight before training\n",
    "        weight_before = first_param.data.clone()\n",
    "\n",
    "        train_loss = train_epoch(\n",
    "            model,\n",
    "            train_loader,\n",
    "            optimizer,\n",
    "            device,\n",
    "            verbose=False,\n",
    "            random_noise_masking=False,\n",
    "        )\n",
    "\n",
    "        # CHECK 3: Print weight after training\n",
    "        weight_after = first_param.data\n",
    "        weight_change = (weight_after - weight_before).abs().mean().item()\n",
    "        print(f\"Weight change: {weight_change:.20f}\")\n",
    "\n",
    "        validation_loss = validate(model, val_loader, device)\n",
    "        training_loss_arr.append(train_loss)\n",
    "        validation_loss_arr.append(validation_loss)\n",
    "        perf.log_epoch(i, train_loss, validation_loss)\n",
    "        if mini >= validation_loss:\n",
    "            mini = validation_loss\n",
    "            early = 0\n",
    "        else:\n",
    "            early += 1\n",
    "        epochs += 1\n",
    "        if early >= stopping_condition:\n",
    "            print(\"Early stop loss\")\n",
    "            break\n",
    "\n",
    "        print(f\"Train Loss: {train_loss:.4f}\")\n",
    "        print(f\"Validation Loss: {validation_loss:.4f}\")\n",
    "\n",
    "        # CHECK 4: Print gradient norms\n",
    "        total_norm = 0\n",
    "        for p in model.parameters():\n",
    "            if p.grad is not None:\n",
    "                total_norm += p.grad.data.norm(2).item() ** 2\n",
    "        total_norm = total_norm**0.5\n",
    "        print(f\"Gradient norm: {total_norm:.6f}\")\n",
    "\n",
    "    training_end = time.time()\n",
    "    total_time = training_end - training_start\n",
    "    perf.finalise(total_time)\n",
    "\n",
    "    print(f\"Training took {total_time} seconds over {epochs} epochs\")\n",
    "    plt.plot(training_loss_arr, label=\"training_loss\", color=\"blue\")\n",
    "    plt.plot(validation_loss_arr, label=\"validation_loss\", color=\"red\")\n",
    "    plt.legend()\n",
    "    plt.savefig(f\"experiments/{experiment_name}/train_loss_plot_{fold}.png\", dpi=300)\n",
    "    plt.close()\n",
    "\n",
    "    torch.save(\n",
    "        model.state_dict(), f\"experiments/{experiment_name}/weather_gnn_best_{fold}.pth\"\n",
    "    )\n",
    "    print(\"✅ model weights saved to weather_gnn_best.pth\")\n",
    "\n",
    "    perf.log_model_parameters(model)\n",
    "    return model\n",
    "\n",
    "\n",
    "model = train(model, train_loader, val_loader, fold=0, device=device)\n",
    "model1 = train(model1, train_loader1, val_loader1, fold=1, device=device)\n",
    "model2 = train(model2, train_loader2, val_loader2, fold=2, device=device)\n",
    "model3 = train(model3, train_loader3, val_loader3, fold=3, device=device)\n",
    "model4 = train(model4, train_loader4, val_loader4, fold=4, device=device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c18fdc00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(next(iter(val_loader))[\"gen_x\"].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "623fbdbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# total_params = sum(param.numel() for param in model.parameters())\n",
    "# print(total_params)\n",
    "# print(list(param for param in model.parameters()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35259636",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(model, dataloader, device, fold=0, verbose=False):\n",
    "    \"\"\"\n",
    "    Test loop following the SAME structure as validate():\n",
    "      - PyG batch format\n",
    "      - x, y shaped [B*N, F]\n",
    "      - mask shaped [B*N]\n",
    "      - edge_index expanded & offset automatically by PyG\n",
    "      - Computes metrics ONLY on test nodes (mask==True)\n",
    "    \"\"\"\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    all_preds = []\n",
    "    all_targets = []\n",
    "    epoch_losses = []\n",
    "\n",
    "    test_bar = tqdm.tqdm(dataloader, desc=\"Testing\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in test_bar:\n",
    "            batch = batch.to(device)\n",
    "\n",
    "            # ----- Extract standard PyG Batch -----\n",
    "            x = batch.x  # [B*N, F]\n",
    "            y = batch.y  # [B*N, target_dim]\n",
    "            mask = batch.mask  # [B*N], boolean\n",
    "            edge_index = batch.edge_index\n",
    "            edge_attr = batch.edge_attr if batch.edge_attr is not None else None\n",
    "\n",
    "            if verbose:\n",
    "                print(f\"x shape: {x.shape}\")\n",
    "                print(f\"y shape: {y.shape}\")\n",
    "                print(f\"mask shape: {mask.shape}\")\n",
    "                print(f\"edge_index shape: {edge_index.shape}\")\n",
    "\n",
    "            assert mask.shape[0] == x.shape[0], (\n",
    "                f\"mask size {mask.shape[0]} != x size {x.shape[0]}\"\n",
    "            )\n",
    "\n",
    "            # ----- Forward pass -----\n",
    "            out = model(x, edge_index, edge_attributes=edge_attr)  # [B*N, out_channels]\n",
    "\n",
    "            # ----- Compute test loss (only masked nodes) -----\n",
    "            loss = F.mse_loss(out[mask], y[mask])\n",
    "            epoch_losses.append(loss.item())\n",
    "\n",
    "            # ----- Collect predictions and targets -----\n",
    "            all_preds.append(out[mask].detach().cpu())\n",
    "            all_targets.append(y[mask].detach().cpu())\n",
    "\n",
    "            test_bar.set_postfix({\"loss\": loss.item()})\n",
    "\n",
    "    # ============================================================\n",
    "    # === CONCATENATE ALL TEST PREDICTIONS & TARGETS (just like validation)\n",
    "    # ============================================================\n",
    "    all_preds = torch.cat(all_preds, dim=0)  # [Total_test_nodes, target_dim]\n",
    "    all_targets = torch.cat(all_targets, dim=0)  # [Total_test_nodes, target_dim]\n",
    "\n",
    "    print(\"Final aggregated prediction shape:\", all_preds.shape)\n",
    "    print(\"Final aggregated target shape:\", all_targets.shape)\n",
    "\n",
    "    # ============================================================\n",
    "    # === Compute Pearson and RMSE (global)\n",
    "    # ============================================================\n",
    "    preds_np = all_preds.numpy().flatten()\n",
    "    targets_np = all_targets.numpy().flatten()\n",
    "\n",
    "    mask = (~np.isnan(preds_np)) & (~np.isnan(targets_np))\n",
    "    pearson_r, pearson_p = pearsonr(targets_np[mask], preds_np[mask])\n",
    "\n",
    "    mse = ((all_preds - all_targets) ** 2).mean()\n",
    "    rmse = torch.sqrt(mse).item()\n",
    "\n",
    "    print(f\"Pearson correlation (Test Nodes): {pearson_r}\")\n",
    "    print(f\"Final Test RMSE: {rmse}\")\n",
    "\n",
    "    # ============================================================\n",
    "    # === Scatter Plot\n",
    "    # ============================================================\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    plt.scatter(targets_np, preds_np, alpha=0.5)\n",
    "\n",
    "    line_max = max(np.nanmax(preds_np), np.nanmax(targets_np))\n",
    "    plt.plot([0, line_max], [0, line_max], \"r--\")\n",
    "\n",
    "    plt.xlabel(\"Actual\")\n",
    "    plt.ylabel(\"Predicted\")\n",
    "    plt.title(\"Test Set Performance\")\n",
    "    plt.grid(True)\n",
    "\n",
    "    text = f\"Pearson r = {pearson_r:.3f}\\nRMSE = {rmse:.3f}\"\n",
    "    plt.text(\n",
    "        0.05,\n",
    "        0.95,\n",
    "        text,\n",
    "        transform=plt.gca().transAxes,\n",
    "        verticalalignment=\"top\",\n",
    "        bbox=dict(facecolor=\"white\", alpha=0.7, edgecolor=\"black\"),\n",
    "    )\n",
    "\n",
    "    plt.savefig(f\"experiments/{experiment_name}/test_scatter_plot_{fold}.png\", dpi=300)\n",
    "    plt.close()\n",
    "    print(\n",
    "        f\"Saved test scatter plot to experiments/{experiment_name}/test_scatter_plot_{fold}.png\"\n",
    "    )\n",
    "\n",
    "    # ============================================================\n",
    "    # === Per-node (per-station) time-series plots\n",
    "    # ============================================================\n",
    "    # If target_dim = 1, reshape to [T, N]\n",
    "    preds_2d = all_preds.reshape(-1, 1)\n",
    "    targets_2d = all_targets.reshape(-1, 1)\n",
    "\n",
    "    # If you want per-station but stations repeat across batches,\n",
    "    # you must have station index stored in batch.\n",
    "    # Here we just plot the single predicted dimension.\n",
    "    for idx in range(preds_2d.shape[1]):\n",
    "        fig, ax = plt.subplots(2, 1, figsize=(15, 8))\n",
    "        ax[0].plot(preds_2d[:, idx])\n",
    "        ax[1].plot(targets_2d[:, idx])\n",
    "        plt.savefig(\n",
    "            f\"experiments/{experiment_name}/station_{idx}_preds_actual_plot_{fold}.png\"\n",
    "        )\n",
    "        plt.close()\n",
    "\n",
    "    return rmse\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ff01a1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loader = prepare_homogeneous_inductive_dataset(\n",
    "    train_graphs[0],\n",
    "    validation_graphs[0],\n",
    "    full_graphs[0],\n",
    "    batch_size=batch_size,\n",
    "    mode=\"test\",\n",
    ")\n",
    "test_loader1 = prepare_homogeneous_inductive_dataset(\n",
    "    train_graphs[1],\n",
    "    validation_graphs[1],\n",
    "    full_graphs[1],\n",
    "    batch_size=batch_size,\n",
    "    mode=\"test\",\n",
    ")\n",
    "test_loader2 = prepare_homogeneous_inductive_dataset(\n",
    "    train_graphs[2],\n",
    "    validation_graphs[2],\n",
    "    full_graphs[2],\n",
    "    batch_size=batch_size,\n",
    "    mode=\"test\",\n",
    ")\n",
    "test_loader3 = prepare_homogeneous_inductive_dataset(\n",
    "    train_graphs[3],\n",
    "    validation_graphs[3],\n",
    "    full_graphs[3],\n",
    "    batch_size=batch_size,\n",
    "    mode=\"test\",\n",
    ")\n",
    "test_loader4 = prepare_homogeneous_inductive_dataset(\n",
    "    train_graphs[4],\n",
    "    validation_graphs[4],\n",
    "    full_graphs[4],\n",
    "    batch_size=batch_size,\n",
    "    mode=\"test\",\n",
    ")\n",
    "\n",
    "RMSE = test_model(model, test_loader, device, fold=0)\n",
    "RMSE1 = test_model(model1, test_loader1, device, fold=1)\n",
    "RMSE2 = test_model(model2, test_loader2, device, fold=2)\n",
    "RMSE3 = test_model(model3, test_loader3, device, fold=3)\n",
    "RMSE4 = test_model(model4, test_loader4, device, fold=4)\n",
    "print(f\"TEST RMSE: {RMSE}\")\n",
    "print(f\"TEST RMSE1: {RMSE1}\")\n",
    "print(f\"TEST RMSE2: {RMSE2}\")\n",
    "print(f\"TEST RMSE3: {RMSE3}\")\n",
    "print(f\"TEST RMSE4: {RMSE4}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f606be2d",
   "metadata": {},
   "source": [
    "# Visualisation of output\n",
    "Test event will be 02-05-2025 0415 to 0615\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb0dc8fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_one_event(test_event_data, radar_features_event, do_plot=True):\n",
    "    \"\"\"\n",
    "    Prepare a single example from `test_event_data` (a pandas slice like\n",
    "    weather_station_df_pivot.iloc[593:602]) and run the model for inference.\n",
    "\n",
    "    This function returns:\n",
    "      gen_out: numpy array of predicted general_station outputs (shape: [num_gen_nodes, out_features])\n",
    "      rain_out: numpy array of predicted rainfall_station outputs (shape: [num_rain_nodes, out_features])\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "\n",
    "    # clone template (so we keep masks/edge_index/order)\n",
    "    test_data = data.clone()\n",
    "\n",
    "    # --- collect station-wise time-series just like you had before ---\n",
    "    test_general_station_data = {}\n",
    "    test_rainfall_station_data = {}\n",
    "\n",
    "    for station in test_event_data.columns.get_level_values(1).unique():\n",
    "        station_cols = (\n",
    "            test_event_data.xs(station, level=1, axis=1)\n",
    "            .interpolate(method=\"linear\")\n",
    "            .fillna(method=\"ffill\")\n",
    "            .fillna(method=\"bfill\")\n",
    "        )\n",
    "        if station in general_station:\n",
    "            test_general_station_data[station] = (\n",
    "                station_cols.values\n",
    "            )  # shape [T, gen_feat]\n",
    "        else:\n",
    "            test_rainfall_station_data[station] = station_cols.values[\n",
    "                :, 0:1\n",
    "            ]  # [T, rain_feat=1]\n",
    "\n",
    "    # Build arrays in the correct node ordering\n",
    "    gen_feats_list = []\n",
    "    rain_feats_list = []\n",
    "\n",
    "    for station in general_station:\n",
    "        gen_feats_list.append(\n",
    "            test_general_station_data[station]\n",
    "        )  # each item: [T, gen_feat_per_t]\n",
    "    for station in rainfall_station:\n",
    "        rain_feats_list.append(\n",
    "            test_rainfall_station_data[station]\n",
    "        )  # each item: [T, rain_feat_per_t]\n",
    "\n",
    "    # Convert to numpy arrays and get shapes\n",
    "    # After np.array(gen_feats_list) => shape [num_gen_nodes, T, gen_feat_per_t]\n",
    "    gen_arr = np.array(gen_feats_list)  # [N_gen, T, Fg]\n",
    "    rain_arr = np.array(rain_feats_list)  # [N_rain, T, Fr]\n",
    "\n",
    "    # --- Convert to the 2-D node-feature format the model expects ---\n",
    "    # There are different sensible choices here:\n",
    "    #  - take last timestep: arr[:, -1, :] -> [N, F]\n",
    "    #  - flatten the time axis into the feature axis: arr.reshape(N, T*F)\n",
    "    # The training/test collate you used produces per-node features (no time dim).\n",
    "    # To match that, we flatten time into features (preserves the whole window).\n",
    "    def flatten_time_axis(arr):\n",
    "        # arr: [N, T, F]\n",
    "        N, T, F = arr.shape\n",
    "        return arr.reshape(N, T * F)  # [N, T*F]\n",
    "\n",
    "    gen_node_feats = gen_arr[:, -1, :].astype(np.float32)\n",
    "    rain_node_feats = rain_arr[:, -1, :].astype(np.float32)  # [N, F]\n",
    "    radar_node_feats = radar_features_event[-1].float()  # [N_radar, F]\n",
    "\n",
    "    # Convert to torch tensors (2-D per node type) and move to device\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    x_dict = {\n",
    "        \"general_station\": torch.tensor(\n",
    "            gen_node_feats, dtype=torch.float, device=device\n",
    "        ),\n",
    "        \"rainfall_station\": torch.tensor(\n",
    "            rain_node_feats, dtype=torch.float, device=device\n",
    "        ),\n",
    "        \"radar_grid\": torch.tensor(radar_node_feats, dtype=torch.float, device=device),\n",
    "    }\n",
    "\n",
    "    # Move edge structures to device (the same ones you used in test_model)\n",
    "    edge_index_dict = {k: v.to(device) for k, v in data.edge_index_dict.items()}\n",
    "    edge_attr_dict = {k: v.to(device) for k, v in data.edge_attr_dict.items()}\n",
    "\n",
    "    # Run model\n",
    "    with torch.no_grad():\n",
    "        out = model(x_dict, edge_index_dict, edge_attr_dict)\n",
    "\n",
    "    # out[...] are torch tensors shaped like [num_nodes, out_features] (depending on your model head)\n",
    "    gen_out = out[\"general_station\"].cpu().numpy()\n",
    "    rain_out = out[\"rainfall_station\"].cpu().numpy()\n",
    "\n",
    "    return gen_out, rain_out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50c852b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_event_data = weather_station_df_pivot.iloc[593:602]  # your 9 timestamps\n",
    "radar_features_event = data[\"radar_grid\"].x[593:602]\n",
    "gen_out, rain_out = visualize_one_event(test_event_data, radar_features_event)\n",
    "out_np = np.concatenate([gen_out, rain_out], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "899f48c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(test_data.edge_index_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ca57f6f",
   "metadata": {},
   "source": [
    "# Visualise rain on radar grid\n",
    "Hard coded to plot only consequitive 9 timestamps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb3f728f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(out_np / 12)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d843bf48",
   "metadata": {},
   "source": [
    "# Visualize Radar Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e460fe47",
   "metadata": {},
   "outputs": [],
   "source": [
    "radar_df = load_radar_dataset(\"sg_radar_data\")\n",
    "\n",
    "visualize_one_radar_image(radar_df=radar_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1993af6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig, ax = plt.subplots(\n",
    "#     3, 3, figsize=(15, 12), subplot_kw={\"projection\": ccrs.PlateCarree()}\n",
    "# )\n",
    "\n",
    "# out_np = out_np / 12\n",
    "# for idx, timestamp in enumerate(out_np):\n",
    "#     output = {}\n",
    "#     count = 0\n",
    "\n",
    "#     for stn in general_station:\n",
    "#         output[stn] = float(timestamp[count])\n",
    "#         count += 1\n",
    "#     for stn in rainfall_station:\n",
    "#         output[stn] = float(timestamp[count])\n",
    "#         count += 1\n",
    "#     axi = ax[idx // 3][idx % 3]\n",
    "#     node_df = pd.Series(output)\n",
    "#     node_df = pandas_to_geodataframe(node_df)\n",
    "#     visualise_gauge_grid(node_df=node_df, ax=axi)\n",
    "#     improved_visualise_radar_grid(\n",
    "#         radar_df.iloc[idx], ax=axi, zoom=bounds_singapore, norm=norm\n",
    "#     )\n",
    "#     visualise_singapore_outline(ax=axi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6af00ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "original_rainfall_rates = (\n",
    "    weather_station_df_pivot.iloc[1773:1797].resample(\"15min\").first()[\"rain_rate\"]\n",
    ")\n",
    "\n",
    "\n",
    "print(original_rainfall_rates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14da652a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a97c8fc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "actual_arr = []\n",
    "pred_arr = []\n",
    "\n",
    "for idx, timestamp in enumerate(out):\n",
    "    output = {}\n",
    "    count = 0\n",
    "    a_arr = []\n",
    "    p_arr = []\n",
    "\n",
    "    for stn in general_station:\n",
    "        output[stn] = float(timestamp[count])\n",
    "        count += 1\n",
    "    for stn in rainfall_station:\n",
    "        output[stn] = float(timestamp[count])\n",
    "        count += 1\n",
    "\n",
    "    for key, value in output.items():\n",
    "        a_arr.append(original_rainfall_rates.iloc[idx][key])\n",
    "        p_arr.append(output[key])\n",
    "    a_arr = list(map(lambda x: float(x), a_arr))\n",
    "    actual_arr.append(a_arr)\n",
    "    pred_arr.append(p_arr)\n",
    "\n",
    "actual_arr = np.array(actual_arr)\n",
    "pred_arr = np.array(pred_arr)\n",
    "\n",
    "print(actual_arr)\n",
    "print(pred_arr)\n",
    "error = []\n",
    "for i in range(len(actual_arr)):\n",
    "    error.append(np.nanmean(actual_arr - pred_arr) ** 2)\n",
    "\n",
    "MSE = np.mean(np.array(error))\n",
    "print(MSE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63ce7ede",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(original_rainfall_rates.iloc[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rainfall",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
